#                                                                     Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§   
# install: pip install --upgrade arabic-reshaper
import arabic_reshaper
#-------------------------------------------------------
# install: pip install python-bidi
from bidi.algorithm import get_display
#-------------------------------------------------------
# pip install transformers datasets accelerate flask
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    AutoModelForTokenClassification,
    AutoModelForQuestionAnswering,
    pipeline
)
import warnings
from flask import Flask, request, jsonify
import time 
import logging
from logging.handlers import RotatingFileHandler
import os

# ============================================================================================
#                             Multiple Tasks Ø¨Ø§ Hugging Face Transformers
# ============================================================================================

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´ ÙØ§Ø±Ø³ÛŒ
def fa(text):
    return get_display(arabic_reshaper.reshape(text))

# -------------------------------------------------------

warnings.filterwarnings('ignore')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª logging Ø¨Ø±Ø§ÛŒ Production
def setup_logging():
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            RotatingFileHandler(
                os.path.join(log_dir, 'nlp_api.log'), 
                maxBytes=10485760,  # 10MB
                backupCount=5
            ),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# Ø¨Ø±Ø±Ø³ÛŒ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

# =====================================================
#  Ø§ÛŒØ¬Ø§Ø¯ Ø³ÛŒØ³ØªÙ… NLP Ú†Ù†Ø¯Ù…Ù†Ø¸ÙˆØ±Ù‡ - Optimized for Production
# =====================================================

class SimpleNLPSystem:
    """Ø³ÛŒØ³ØªÙ… NLP Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ load Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯"""
    def analyze_text(self, text):
        return {
            'sentiment': {'sentiment': 'Neutral', 'confidence': 0.5, 'probabilities': [0.5, 0.5]},
            'entities': [],
            'summary': text[:100] + '...' if len(text) > 100 else text,
            'note': 'Using simple NLP system - main models not loaded'
        }

class AdvancedNLPSystem:
    def __init__(self):
        self.models = {}
        self.tokenizers = {}
        self.logger = logging.getLogger(__name__)
        self._load_models()
    
    def _load_models(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Production"""
        self.logger.info("Loading models for production...")
        
        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² crash
            SMALL_MODELS = {
                'classification': 'distilbert-base-uncased',  # Ù…Ø¯Ù„ Ú©ÙˆÚ†Ú©â€ŒØªØ±
                'ner': 'dbmdz/bert-small-cased-finetuned-conll03-english',  # Ù…Ø¯Ù„ Ú©ÙˆÚ†Ú©â€ŒØªØ±
            }
            
            # Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ classification - Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©ÙˆÚ†Ú©
            self.logger.info("Loading classification model...")
            self.models['classification'] = AutoModelForSequenceClassification.from_pretrained(
                SMALL_MODELS['classification'], 
                num_labels=2,
                cache_dir="./model_cache"
            )
            self.tokenizers['classification'] = AutoTokenizer.from_pretrained(
                SMALL_MODELS['classification'],
                cache_dir="./model_cache"
            )
            
            # Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ NER - Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©ÙˆÚ†Ú©
            self.logger.info("Loading NER model...")
            self.models['ner'] = AutoModelForTokenClassification.from_pretrained(
                SMALL_MODELS['ner'],
                cache_dir="./model_cache"
            )
            self.tokenizers['ner'] = AutoTokenizer.from_pretrained(
                SMALL_MODELS['ner'],
                cache_dir="./model_cache"
            )
            
            # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ GPU
            for name, model in self.models.items():
                model.to(device)
                model.eval()  # Ø­Ø§Ù„Øª evaluation Ø¨Ø±Ø§ÛŒ production
            
            self.logger.info("All models loaded successfully!")
            
        except Exception as e:
            self.logger.error(f"Error loading models: {e}")
            # Ø¨Ù‡ Ø¬Ø§ÛŒ raiseØŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            self.models = {}
            self.tokenizers = {}
    
    def analyze_text(self, text):
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ú©Ø§Ù…Ù„ Ù…ØªÙ† Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
        try:
            # Ø§Ú¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ load Ù†Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø§Ø² Ø³ÛŒØ³ØªÙ… Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            if not self.models:
                simple_system = SimpleNLPSystem()
                return simple_system.analyze_text(text)
                
            results = {}
            
            # Sentiment Analysis
            results['sentiment'] = self._analyze_sentiment(text)
            
            # Named Entity Recognition
            results['entities'] = self._extract_entities(text)
            
            # Text Summary
            results['summary'] = self._generate_summary(text)
            
            return results
            
        except Exception as e:
            self.logger.error(f"Error in text analysis: {e}")
            return {'error': str(e)}
    
    def _analyze_sentiment(self, text):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        try:
            if 'classification' not in self.models:
                return {'sentiment': 'Neutral', 'confidence': 0.5, 'error': 'Model not loaded'}
                
            inputs = self.tokenizers['classification'](
                text, 
                return_tensors="pt", 
                truncation=True, 
                padding=True,
                max_length=128  # Ú©Ø§Ù‡Ø´ Ø·ÙˆÙ„ Ø¨Ø±Ø§ÛŒ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø¯Ø± memory
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.models['classification'](**inputs)
            
            probs = torch.softmax(outputs.logits, dim=1)
            sentiment = "Positive" if torch.argmax(probs) == 1 else "Negative"
            
            confidence = float(probs[0][torch.argmax(probs)].item())
            probabilities = [float(prob) for prob in probs.cpu().numpy()[0]]
            
            return {
                'sentiment': sentiment,
                'confidence': confidence,
                'probabilities': probabilities
            }
            
        except Exception as e:
            self.logger.error(f"Sentiment analysis error: {e}")
            return {'sentiment': 'Unknown', 'confidence': 0.0, 'error': str(e)}
    
    def _extract_entities(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ Ø¨Ø§ pipeline Ø¨Ù‡ÛŒÙ†Ù‡"""
        try:
            if 'ner' not in self.models:
                return []
                
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² pipeline Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡
            ner_pipeline = pipeline(
                "token-classification",
                model=self.models['ner'],
                tokenizer=self.tokenizers['ner'],
                aggregation_strategy="simple",
                device=0 if torch.cuda.is_available() else -1
            )
            
            entities = ner_pipeline(text[:512])  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ù…ØªÙ†
            
            for entity in entities:
                entity['score'] = float(entity['score'])
            
            return entities
            
        except Exception as e:
            self.logger.error(f"NER extraction error: {e}")
            return []
    
    def _generate_summary(self, text):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø®Ù„Ø§ØµÙ‡ Ù…ØªÙ†"""
        try:
            sentences = [s.strip() for s in text.split('.') if s.strip()]
            if len(sentences) > 3:
                return '. '.join(sentences[:2]) + '.'
            return text
        except Exception as e:
            self.logger.error(f"Summary generation error: {e}")
            return text[:200] + "..." if len(text) > 200 else text

# =====================================================
#  Flask API Application - Optimized for IIS
# =====================================================

class NLPAPI:
    def __init__(self):
        self.app = Flask(__name__)
        self.app.config['JSON_SORT_KEYS'] = False
        self.app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False
        self.logger = logging.getLogger(__name__)
        
        # Lazy loading - Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ù‡Ù†Ú¯Ø§Ù… Ø§ÙˆÙ„ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øª load Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
        self.nlp_system = None
        self._setup_routes()
        self._setup_error_handlers()
    
    def _get_nlp_system(self):
        """Lazy loading Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ Ø¯Ø± startup"""
        if self.nlp_system is None:
            try:
                self.logger.info("Loading NLP system...")
                self.nlp_system = AdvancedNLPSystem()
                self.logger.info("NLP system loaded successfully")
            except Exception as e:
                self.logger.error(f"Failed to load NLP system: {e}")
                self.nlp_system = SimpleNLPSystem()
        return self.nlp_system
    
    def _setup_routes(self):
        """ØªÙ†Ø¸ÛŒÙ… routes Ø¨Ø§ CORS support"""
        
        @self.app.route('/analyze', methods=['POST', 'OPTIONS'])
        def analyze_text():
            if request.method == 'OPTIONS':
                return self._build_cors_response()
                
            data = request.get_json(silent=True) or {}
            text = data.get('text', '')
            
            if not text:
                return jsonify({'error': 'No text provided'}), 400
            
            try:
                self.logger.info(f"Analyzing text: {text[:100]}...")
                nlp_system = self._get_nlp_system()  # Lazy loading
                results = nlp_system.analyze_text(text)
                response = jsonify(results)
                return self._add_cors_headers(response)
                
            except Exception as e:
                self.logger.error(f"API error: {e}")
                return jsonify({'error': 'Internal server error'}), 500
        
        @self.app.route('/health', methods=['GET'])
        def health_check():
            return jsonify({
                'status': 'healthy', 
                'device': str(device),
                'timestamp': time.time()
            })
        
        @self.app.route('/', methods=['GET'])
        def home():
            return jsonify({
                'message': 'NLP API Server is Running',
                'version': '1.0.0',
                'endpoints': {
                    'analyze': 'POST /analyze',
                    'health': 'GET /health'
                }
            })
    
    def _setup_error_handlers(self):
        """Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§"""
        
        @self.app.errorhandler(404)
        def not_found(error):
            return jsonify({'error': 'Endpoint not found'}), 404
        
        @self.app.errorhandler(500)
        def internal_error(error):
            self.logger.error(f"Internal server error: {error}")
            return jsonify({'error': 'Internal server error'}), 500
    
    def _build_cors_response(self):
        """Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ CORS preflight"""
        response = jsonify({'status': 'ok'})
        return self._add_cors_headers(response)
    
    def _add_cors_headers(self, response):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† headers Ø¨Ø±Ø§ÛŒ CORS"""
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
        response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')
        return response
    
    def run(self, host='0.0.0.0', port=5000, debug=False):
      # ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† debug Ø¯Ø± production
      self.app.run(
          host=host, 
          port=port, 
          debug=False,  # ØªØºÛŒÛŒØ± Ø¨Ù‡ False
          threaded=True)

# =====================================================
#  WSGI Configuration for IIS
# =====================================================

# Ø§ÛŒØ¬Ø§Ø¯ application instance Ø¨Ø±Ø§ÛŒ IIS
def create_app():
    """Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø¨Ú© Ø¨Ø±Ø§ÛŒ IIS"""
    api = NLPAPI()
    return api.app

# Global application instance - Ø§ÛŒÙ† Ù…ØªØºÛŒØ± Ø¨Ø§ÛŒØ¯ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
application = create_app()

# =====================================================
#  Development Server
# =====================================================

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ¤– NLP API Server - Development Mode")
    print("=" * 60)
    
    api = NLPAPI()
    api.run(debug=True, port=5000)