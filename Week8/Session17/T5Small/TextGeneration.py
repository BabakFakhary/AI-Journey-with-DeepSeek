#                                                                    Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

# ===========================================================================
#  ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† (Text Generation)
# ===========================================================================

tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

def generate_text(prompt, max_length=100):
    """ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø±Ø§Ù…Ù¾Øª"""
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_beams=3,
            temperature=0.9,
            do_sample=True,
            early_stopping=True
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ: ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø®Ù„Ø§Ù‚
prompts = [
    "The future of artificial intelligence",
    "How to learn programming",
    "The benefits of renewable energy"
]

print("ðŸ§  Creative text production:")
for prompt in prompts:
    generated = generate_text(prompt)
    print(f"Prompt:  {prompt}")
    print(f"Creative:  {generated}")
    print("-" * 50)