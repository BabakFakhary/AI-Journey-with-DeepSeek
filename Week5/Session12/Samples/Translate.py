#                                                                   Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§
#                                                                   Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§
# install: pip install --upgrade arabic-reshaper
import arabic_reshaper
#-------------------------------------------------------
# install: pip install python-bidi
from bidi.algorithm import get_display
#-------------------------------------------------------
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

def fa(text):
    return get_display(arabic_reshaper.reshape(text)) 

print(fa("ğŸ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù†Ø¯"))

# -------------------------------------------------------------------------------------------------
#  ÛŒÚ© Ø¯ÛŒØªØ§Ø³Øª Ú©ÙˆÚ†Ú© Ø§Ø² Ø¬Ù…Ù„Ø§Øª Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ùˆ Ù…Ø¹Ø§Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ Ø¢Ù†Ù‡Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
#  Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…ÙÙ‡ÙˆÙ… ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ú©Ø§ÙÛŒ Ø§Ø³Øª 
# Ù…Ø¯Ù„ ÙØ¹Ù„ÛŒ ÙÙ‚Ø· ÛŒÚ© Ú©Ù„Ù…Ù‡ ØªØ±Ø¬Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
# -------------------------------------------------------------------------------------------------

# =============================================================================
# 1. Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª Ù†Ù…ÙˆÙ†Ù‡ (Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ)
# =============================================================================
print("\n" + "="*60)
print(fa("1. Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª Ù†Ù…ÙˆÙ†Ù‡ (Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ)"))
print("="*60)

# Ø¯ÛŒØªØ§Ø³Øª Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…ÙÙ‡ÙˆÙ…
english_sentences = [
    "I love machine learning",
    "Attention is important",
    "Transformers are powerful",
    "Neural networks learn",
    "Deep learning is amazing",
    "AI is the future",
    "Natural language processing",
    "Computer vision applications",
    "Hello world program",
    "GPT models are large"
]

persian_sentences = [
    "Ù…Ù† Ø¹Ø§Ø´Ù‚ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ù‡Ø³ØªÙ…",
    "ØªÙˆØ¬Ù‡ Ù…Ù‡Ù… Ø§Ø³Øª", 
    "ØªØ±Ø§Ù†Ø³ÙÙˆØ±Ù…Ø±Ù‡Ø§ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ù‡Ø³ØªÙ†Ø¯",
    "Ø´Ø¨Ú©Ù‡ Ù‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ ÛŒØ§Ø¯ Ù…ÛŒ Ú¯ÛŒØ±Ù†Ø¯",
    "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ Ø´Ú¯ÙØª Ø§Ù†Ú¯ÛŒØ² Ø§Ø³Øª",
    "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡ Ø§Ø³Øª",
    "Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ",
    "Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±",
    "Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§",
    "Ù…Ø¯Ù„ Ù‡Ø§ÛŒ Ø¬ÛŒ Ù¾ÛŒ ØªÛŒ Ø¨Ø²Ø±Ú¯ Ù‡Ø³ØªÙ†Ø¯"
]

print(fa("ğŸ“Š Ø¯ÛŒØªØ§Ø³Øª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯:"))
for i, (eng, per) in enumerate(zip(english_sentences, persian_sentences)):
    print(f"{i+1:2d}. EN: {eng:<30} FA: {fa(per)}")

# =============================================================================
# 2. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ùˆ Tokenization
  # Ø§Ø² Tokenizer keras Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
  # oov_token="<OOV>" Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
  # word_index Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ mapping Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ø³Øª
# =============================================================================
print("\n" + "="*60)
print(fa("2. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ùˆ Tokenization"))
print("="*60)

# ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø±Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
  # ÛŒØ§ÙØªÙ† Ú©Ù„Ù…Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
eng_tokenizer = tf.keras.preprocessing.text.Tokenizer(
    filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',  # ÙÛŒÙ„ØªØ± Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ
    oov_token="<OOV>"  # ØªÙˆÚ©Ù† Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡
)
eng_tokenizer.fit_on_texts(english_sentences)
eng_vocab_size = len(eng_tokenizer.word_index) + 1  # +1 Ø¨Ø±Ø§ÛŒ padding

# ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
  # ÛŒØ§ÙØªÙ† Ú©Ù„Ù…Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
per_tokenizer = tf.keras.preprocessing.text.Tokenizer(
    filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', # ÙÛŒÙ„ØªØ± Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ
    oov_token="<OOV>" # ØªÙˆÚ©Ù† Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡
)
per_tokenizer.fit_on_texts(persian_sentences)
per_vocab_size = len(per_tokenizer.word_index) + 1

print(fa(f"ğŸ“ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø§ÛŒØ±Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: {eng_vocab_size}"))
print(fa(f"ğŸ“ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø§ÛŒØ±Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ: {per_vocab_size}"))
print(fa(f"ğŸ“‹ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: {list(eng_tokenizer.word_index.keys())[:10]}..."))
print(fa(f"ğŸ“‹ ÙˆØ§Ú˜Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ: {list(per_tokenizer.word_index.keys())[:10]}..."))

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø¹Ø¯Ø§Ø¯
  # english_sentences = ["I love machine learning"]  
  # eng_tokenizer.word_index = { 'machine': 1, 'learning': 2, 'love': 3, 'i': 4, 'attention': 5, 'is': 6, 'important': 7, 'transformers': 8, 'are': 9, 'powerful': 10}
  # Ø®Ø±ÙˆØ¬ÛŒ : # "I love machine learning" â†’ [4, 3, 1, 2]
eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)
per_sequences = per_tokenizer.texts_to_sequences(persian_sentences)

print(fa(f"\nğŸ”¢ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: {eng_sequences[0]}"))
print(fa(f"ğŸ”¢ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ: {per_sequences[0]}"))

# =============================================================================
# 3. Padding Ø¨Ø±Ø§ÛŒ ÛŒÚ©Ø³Ø§Ù† Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§
# =============================================================================
print("\n" + "="*60)
print(fa("3. Padding Ø¨Ø±Ø§ÛŒ ÛŒÚ©Ø³Ø§Ù† Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§"))
print("="*60)

max_len = 10  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡

eng_padded = tf.keras.preprocessing.sequence.pad_sequences(
    eng_sequences, maxlen=max_len, padding='post', truncating='post')

per_padded = tf.keras.preprocessing.sequence.pad_sequences(
    per_sequences, maxlen=max_len, padding='post', truncating='post')

print(fa(f"ğŸ“ Ø·ÙˆÙ„ Ù¾Ø³ Ø§Ø² padding: {max_len}"))
print(fa(f"ğŸ“Š Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: {eng_padded.shape}"))
print(fa(f"ğŸ“Š Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ: {per_padded.shape}"))

print(fa(f"\nğŸ”¤ Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ù…Ù„Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù¾Ø³ Ø§Ø² padding: {eng_padded[0]}"))
print(fa(f"ğŸ”¤ Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ù…Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ Ù¾Ø³ Ø§Ø² padding: {per_padded[0]}"))

# =============================================================================
# 4. ØªØ¹Ø±ÛŒÙ Ù„Ø§ÛŒÙ‡ ØªÙˆØ¬Ù‡ Ú†Ù†Ø¯Ø³Ø±ÛŒ (Multi-Head Attention)
# =============================================================================
print("\n" + "="*60)
print(fa("4. ØªØ¹Ø±ÛŒÙ Ù„Ø§ÛŒÙ‡ ØªÙˆØ¬Ù‡ Ú†Ù†Ø¯Ø³Ø±ÛŒ (Multi-Head Attention)"))
print("="*60)

class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        
        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ d_model Ø¨Ø± num_heads Ø¨Ø®Ø´ Ù¾Ø°ÛŒØ± Ø§Ø³Øª
        assert d_model % num_heads == 0
        
        self.depth = d_model // num_heads  # Ø¹Ù…Ù‚ Ù‡Ø± Ø³Ø± ØªÙˆØ¬Ù‡
        
        # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Dense Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Q, K, V
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        
        # Ù„Ø§ÛŒÙ‡ Dense Ù†Ù‡Ø§ÛŒÛŒ
        self.dense = tf.keras.layers.Dense(d_model)
        
    def split_heads(self, x, batch_size):
        """ØªÙ‚Ø³ÛŒÙ… Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø¹Ø¯ Ø¨Ù‡ (num_heads, depth)"""
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def call(self, q, k, v, mask=None):
        batch_size = tf.shape(q)[0]
        
        # ØªÙˆÙ„ÛŒØ¯ Q, K, V
        q = self.wq(q)  # (batch_size, seq_len, d_model)
        k = self.wk(k)  # (batch_size, seq_len, d_model)
        v = self.wv(v)  # (batch_size, seq_len, d_model)
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ø³Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)
        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)
        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙˆØ¬Ù‡
        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)
        
        # Ø§Ø¯ØºØ§Ù… Ø³Ø±Ù‡Ø§
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)
        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)
        
        # Ù„Ø§ÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ
        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)
        
        return output, attention_weights
    
    def scaled_dot_product_attention(self, q, k, v, mask):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙˆØ¬Ù‡ Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ Ù…Ù‚ÛŒØ§Ø³â€ŒØ´Ø¯Ù‡"""
        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)
        
        # Ù…Ù‚ÛŒØ§Ø³ Ú©Ø±Ø¯Ù†
        dk = tf.cast(tf.shape(k)[-1], tf.float32)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
        
        # Ø§Ø¹Ù…Ø§Ù„ Ù…Ø§Ø³Ú© Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
        if mask is not None:
            scaled_attention_logits += (mask * -1e9)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆØ¬Ù‡
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)
        
        # Ø¶Ø±Ø¨ Ø¯Ø± Ù…Ù‚Ø§Ø¯ÛŒØ±
        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)
        
        return output, attention_weights

print(fa("âœ… Ù„Ø§ÛŒÙ‡ MultiHeadAttention ØªØ¹Ø±ÛŒÙ Ø´Ø¯"))

# =============================================================================
# 5. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡
# =============================================================================
print("\n" + "="*60)
print(fa("5. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡"))
print("="*60)

# Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ùˆ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
class SimpleTranslationModel(tf.keras.Model):
    def __init__(self, vocab_size_eng, vocab_size_per, d_model):
        super(SimpleTranslationModel, self).__init__()
        
        self.eng_embedding = tf.keras.layers.Embedding(vocab_size_eng, d_model)
        self.per_embedding = tf.keras.layers.Embedding(vocab_size_per, d_model)
        
        self.attention = MultiHeadAttention(d_model, 2)
        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()
        self.dense1 = tf.keras.layers.Dense(64, activation='relu')
        self.dense2 = tf.keras.layers.Dense(vocab_size_per, activation='softmax')
        self.dropout = tf.keras.layers.Dropout(0.1)
        
    def call(self, inputs):
        eng_input, per_input = inputs
        
        # Embedding
        eng_embedded = self.eng_embedding(eng_input)
        per_embedded = self.per_embedding(per_input)
        
        # ØªÙˆØ¬Ù‡
        context, _ = self.attention(per_embedded, eng_embedded, eng_embedded)
        context = self.dropout(context)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        pooled = self.global_pool(context)
        x = self.dense1(pooled)
        x = self.dropout(x)
        output = self.dense2(x)
        
        return output

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„
vocab_size_eng = eng_vocab_size
vocab_size_per = per_vocab_size
d_model = 64  # Ø¨Ø¹Ø¯ embedding Ùˆ hidden states

print(fa(f"ğŸ”§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„:"))
print(fa(f"   - vocab_size_eng: {vocab_size_eng}"))
print(fa(f"   - vocab_size_per: {vocab_size_per}"))
print(fa(f"   - d_model: {d_model}"))

# Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
model = SimpleTranslationModel(eng_vocab_size, per_vocab_size, d_model)
print(fa("âœ… Ù…Ø¯Ù„ ØªØ±Ø¬Ù…Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯"))

# =============================================================================
# 6. ØªØ³Øª Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
# =============================================================================
print("\n" + "="*60)
print(fa("6. ØªØ³Øª Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡"))
print("="*60)

# Ø§Ù†ØªØ®Ø§Ø¨ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
sample_idx = 0
# Ø§ÛŒÙ† Ø®Ø· Ú©Ø¯ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ± ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø§Ø´Ø¯.
sample_eng = tf.constant([eng_padded[sample_idx]])  # Ø¬Ù…Ù„Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
sample_per = tf.constant([per_padded[sample_idx]])   # Ø¬Ù…Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ

print(fa(f"ğŸ” ØªØ³Øª Ù…Ø¯Ù„ Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡ {sample_idx + 1}:"))
print(fa(f"   EN: {english_sentences[sample_idx]}"))
print(fa(f"   FA: {persian_sentences[sample_idx]}"))
print(fa(f"   EN tokens: {sample_eng.numpy()[0]}")) # Ø§ÛŒÙ† Ø®Ø· Ú©Ø¯ ÛŒÚ© ØªØ§Ù†Ø³ÙˆØ± Ø±Ø§ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡ Ù†Ø§Ù… Ù¾Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯Ù‡ Ùˆ Ø§ÙˆÙ„ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¢Ø±Ø§ÛŒÙ‡ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
print(fa(f"   FA tokens: {sample_per.numpy()[0]}")) # Ø§ÛŒÙ† Ø®Ø· Ú©Ø¯ ÛŒÚ© ØªØ§Ù†Ø³ÙˆØ± Ø±Ø§ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡ Ù†Ø§Ù… Ù¾Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯Ù‡ Ùˆ Ø§ÙˆÙ„ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¢Ø±Ø§ÛŒÙ‡ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.

# Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„
output = model([sample_eng, sample_per])

print(fa(f"\nğŸ“Š Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„:"))
print(fa(f"   Output shape: {output.shape}"))
print(fa(f"   Ù†Ù…ÙˆÙ†Ù‡ Ø®Ø±ÙˆØ¬ÛŒ: {output.numpy()[0, :5]}..."))  # 5 Ù…Ù‚Ø¯Ø§Ø± Ø§ÙˆÙ„

# =============================================================================
# 7. Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
# =============================================================================
print("\n" + "="*60)
print(fa("7. Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„"))
print("="*60)

# Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print(fa("âœ… Ù…Ø¯Ù„ Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ø´Ø¯"))

# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ validation
X_train, X_val, y_train, y_val = train_test_split(
    eng_padded, per_padded, test_size=0.2, random_state=42
)

# ØªØºÛŒÛŒØ± Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ù Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ø¨Ù‚Øª Ø¨Ø§ Ù…Ø¯Ù„
# Ù…Ø¯Ù„ Ù…Ø§ ÛŒÚ© Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (not per token)
# Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø¨Ø§ÛŒØ¯ target Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ Ù…Ù†Ø§Ø³Ø¨ Ø¯Ø±Ø¢ÙˆØ±ÛŒÙ…

# Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ sequence-to-vectorØŒ Ø¨Ø§ÛŒØ¯ target Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ (batch_size,) Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…
# Ù…Ø§ Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† ØªÙˆÚ©Ù† ØºÛŒØ±ØµÙØ± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† target Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
def get_first_nonzero_token(sequences):
    targets = []
    for seq in sequences:
        for token in seq:
            if token != 0:
                targets.append(token)
                break
        else:
            targets.append(0)  # Ø§Ú¯Ø± Ù‡Ù…Ù‡ ØµÙØ± Ø¨ÙˆØ¯Ù†Ø¯
    return np.array(targets)

y_train_target = get_first_nonzero_token(y_train)
y_val_target = get_first_nonzero_token(y_val)

print(fa(f"ğŸ“Š Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: {X_train.shape[0]} Ù†Ù…ÙˆÙ†Ù‡"))
print(fa(f"ğŸ“Š Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ validation: {X_val.shape[0]} Ù†Ù…ÙˆÙ†Ù‡"))
print(fa(f"ğŸ“Š Ø´Ú©Ù„ y_train_target: {y_train_target.shape}"))
print(fa(f"ğŸ“Š Ø´Ú©Ù„ y_val_target: {y_val_target.shape}"))

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
print(fa("\nğŸ¯ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„..."))
history = model.fit(
    [X_train, y_train],  # ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ encoder Ùˆ decoder
    y_train_target,      # Ù‡Ø¯Ùâ€ŒÙ‡Ø§
    epochs=50,
    batch_size=4,
    validation_data=(
        [X_val, y_val],
        y_val_target
    ),
    verbose=1
)

print(fa("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„ Ø´Ø¯"))

# =============================================================================
# 8. ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡
# =============================================================================
print("\n" + "="*60)
print(fa("8. ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡"))
print("="*60)

def simple_translate(model, sentence, eng_tokenizer, per_tokenizer, max_len=10):
    """ØªØ±Ø¬Ù…Ù‡ Ø³Ø§Ø¯Ù‡ ÛŒÚ© Ø¬Ù…Ù„Ù‡"""
    try:
        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ù‡ ÙˆØ±ÙˆØ¯ÛŒ
        sequence = eng_tokenizer.texts_to_sequences([sentence])
        padded = tf.keras.preprocessing.sequence.pad_sequences(
            sequence, maxlen=max_len, padding='post')
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ø®Ø§Ù„ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ decoder
        empty_decoder_input = tf.constant([[0] * max_len])
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        predictions = model([tf.constant(padded), empty_decoder_input])
        predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ØªÙ†
        if predicted_id > 0 and predicted_id < per_vocab_size:
            word = per_tokenizer.index_word.get(predicted_id, '')
            return word if word and word != '<OOV>' else "ØªØ±Ø¬Ù…Ù‡ Ù†Ø§Ù…ÙˆÙÙ‚"
        
        return "ØªØ±Ø¬Ù…Ù‡ Ù†Ø§Ù…ÙˆÙÙ‚"
    
    except Exception as e:
        return f"Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡: {e}"

# ØªØ³Øª ØªØ§Ø¨Ø¹ ØªØ±Ø¬Ù…Ù‡
test_sentence = "I love machine learning"
translated = simple_translate(model, test_sentence, eng_tokenizer, per_tokenizer)

print(fa(f"ğŸ”¤ Ø¬Ù…Ù„Ù‡ ØªØ³Øª: '{test_sentence}'"))
print(fa(f"ğŸŒ ØªØ±Ø¬Ù…Ù‡ Ù…Ø¯Ù„: '{translated}'"))
print(fa(f"ğŸ”¤ ØªØ±Ø¬Ù…Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ: 'Ù…Ù† Ø¹Ø§Ø´Ù‚ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ù‡Ø³ØªÙ…'"))

# =============================================================================
# 9. Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬ Ùˆ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
# =============================================================================
print("\n" + "="*60)
print(fa("9. Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬ Ùˆ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§"))
print("="*60)

# Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ù‚Øª Ùˆ loss
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label=fa('Ø¯Ù‚Øª Ø¢Ù…ÙˆØ²Ø´'))
plt.plot(history.history['val_accuracy'], label=fa('Ø¯Ù‚Øª validation'))
plt.title(fa('Ø¯Ù‚Øª Ù…Ø¯Ù„'))
plt.xlabel(fa('Ø¯ÙˆØ±Ù‡'))
plt.ylabel(fa('Ø¯Ù‚Øª'))
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label=fa('Ø®Ø·Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´'))
plt.plot(history.history['val_loss'], label=fa('Ø®Ø·Ø§ÛŒ validation'))
plt.title(fa('Ø®Ø·Ø§ÛŒ Ù…Ø¯Ù„'))
plt.xlabel(fa('Ø¯ÙˆØ±Ù‡'))
plt.ylabel(fa('Ø®Ø·Ø§'))
plt.legend()

plt.tight_layout()
plt.show()

# =============================================================================
# 10. ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¬Ø¯ÛŒØ¯
# =============================================================================
print("\n" + "="*60)
print(fa("10. ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¬Ø¯ÛŒØ¯"))
print("="*60)

test_sentences = [
    "Attention is important",
    "Neural networks learn",
    "AI is the future"
]

print(fa("ğŸ§ª ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¬Ø¯ÛŒØ¯:"))
for sentence in test_sentences:
    translated = simple_translate(model, sentence, eng_tokenizer, per_tokenizer)
    print(fa(f"   EN: {sentence}"))
    print(fa(f"   FA: {translated}"))
    print(fa(f"   {'-'*40}"))

print(fa("\nğŸ‰ Ù¾Ø±ÙˆÚ˜Ù‡ ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ú©Ø§Ù…Ù„ Ø´Ø¯!"))