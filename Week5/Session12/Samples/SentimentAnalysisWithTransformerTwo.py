#                                                                   Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§
# install: pip install --upgrade arabic-reshaper
import arabic_reshaper
#-------------------------------------------------------
# install: pip install python-bidi
from bidi.algorithm import get_display
#-------------------------------------------------------
import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import re
import json
import matplotlib.pyplot as plt

# ---------------------------------------------------------------------------------------------------
# ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ØªØ´Ø®ÛŒØµ Ø¬Ù…Ù„Ø§Øª Ø®Ù†Ø«ÛŒ
# ---------------------------------------------------------------------------------------------------

def fa(text):
    return get_display(arabic_reshaper.reshape(text)) 

# --------------------------------------------------------------------
# 1. ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙØ§Ø±Ø³ÛŒ (Ù†Ø¸Ø±Ø§Øª Ù…Ø­ØµÙˆÙ„Ø§Øª) Ø¨Ø§ Ú©Ù„Ø§Ø³ Ø®Ù†Ø«ÛŒ
# --------------------------------------------------------------------
# Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù†Ø¸Ø±Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒ ÙØ§Ø±Ø³ÛŒ
reviews_data = [
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª (Ø§ÙØ²ÙˆØ¯Ù‡ Ø´Ø¯Ù‡)
    {"text": "Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒ Ù‡Ø³ØªÙ…", "label": 1},
    {"text": "Ú©ÛŒÙÛŒØª Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ø¯", "label": 1},
    {"text": "Ø®Ø±ÛŒØ¯ Ø®ÙˆØ¨ÛŒ Ø¨ÙˆØ¯ ØªÙˆØµÛŒÙ‡ Ù…ÛŒ Ú©Ù†Ù…", "label": 1},
    {"text": "Ø¹Ù…Ù„Ú©Ø±Ø¯ ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ø¯", "label": 1},
    {"text": "Ø§Ø±Ø²Ø´ Ø®Ø±ÛŒØ¯ Ø¯Ø§Ø±Ø¯", "label": 1},    
    {"text": "Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯", "label": 1},   
    {"text": "Ø®ÙˆØ¨ Ø¨ÙˆØ¯", "label": 1},   
    {"text": "Ø¹Ø§Ù„ÛŒ", "label": 1}, 
    {"text": "Ø®ÙˆØ¨", "label": 1}, 
    {"text": "Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ ÙˆØ§Ù‚Ø¹Ø§ Ø¹Ø§Ù„ÛŒ Ø§Ø³Øª Ùˆ Ú©ÛŒÙÛŒØª ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ø¯", "label": 1},  # Ù…Ø«Ø¨Øª
    {"text": "Ù‚ÛŒÙ…Øª Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¯Ø§Ø±Ø¯ Ùˆ Ø§Ø±Ø²Ø´ Ø®Ø±ÛŒØ¯ Ø¯Ø§Ø±Ø¯", "label": 1},  # Ù…Ø«Ø¨Øª
    {"text": "Ø³Ø±ÛŒØ¹ Ø±Ø³ÛŒØ¯ Ùˆ Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯", "label": 1},  # Ù…Ø«Ø¨Øª
    {"text": "Ú©ÛŒÙÛŒØª Ø³Ø§Ø®Øª Ø¹Ø§Ù„ÛŒØŒ Ú©Ø§Ù…Ù„Ø§ Ø±Ø§Ø¶ÛŒ Ù‡Ø³ØªÙ…", "label": 1},  # Ù…Ø«Ø¨Øª
    {"text": "Ú©Ø§Ø±Ø§ÛŒÛŒ ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒ Ú©Ù†Ù…", "label": 1},  # Ù…Ø«Ø¨Øª
    {"text": "Ø·Ø±Ø§Ø­ÛŒ Ø²ÛŒØ¨Ø§ Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¹Ø§Ù„ÛŒ", "label": 1},  # Ù…Ø«Ø¨Øª
    {"text": "Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‚ÛŒÙ…ØªØ´ Ø¹Ø§Ù„ÛŒÙ‡", "label": 1},  # Ù…Ø«Ø¨Øª
    {"text": "Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ø±Ùˆ Ø¯Ø§Ø±Ù‡", "label": 1},  # Ù…Ø«Ø¨Øª
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù†ÙÛŒ (Ø§ÙØ²ÙˆØ¯Ù‡ Ø´Ø¯Ù‡)
    {"text": "Ø®ÛŒÙ„ÛŒ Ø¨Ø¯ Ø¨ÙˆØ¯ Ù†Ø§Ø±Ø§Ø¶ÛŒ Ù‡Ø³ØªÙ…", "label": 0},
    {"text": "Ú©ÛŒÙÛŒØª Ø¨Ø³ÛŒØ§Ø± Ù¾Ø§ÛŒÛŒÙ†ÛŒ Ø¯Ø§Ø±Ø¯", "label": 0},
    {"text": "Ù¾Ø´ÛŒÙ…Ø§Ù† Ø´Ø¯Ù… Ø§Ø² Ø®Ø±ÛŒØ¯", "label": 0},
    {"text": "Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙÛŒ Ø¯Ø§Ø±Ø¯", "label": 0},
    {"text": "Ø§Ø±Ø²Ø´ Ø®Ø±ÛŒØ¯ Ù†Ø¯Ø§Ø±Ø¯", "label": 0},   
    {"text": "Ø®ÛŒÙ„ÛŒ Ø¨Ø¯ Ùˆ Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø¨ÙˆØ¯ØŒ Ù¾Ø´ÛŒÙ…Ø§Ù† Ø´Ø¯Ù…", "label": 0},  # Ù…Ù†ÙÛŒ    
    {"text": "Ø§ØµÙ„Ø§ ØªÙˆØµÛŒÙ‡ Ù†Ù…ÛŒ Ú©Ù†Ù…ØŒ waste of money", "label": 0},  # Ù…Ù†ÙÛŒ    
    {"text": "Ù…Ø­ØµÙˆÙ„ Ù…Ø¹ÛŒÙˆØ¨ Ø±Ø³ÛŒØ¯ØŒ Ø¨Ø³ÛŒØ§Ø± Ù†Ø§Ø±Ø§Ø­ØªÙ…", "label": 0},  # Ù…Ù†ÙÛŒ    
    {"text": "Ø¨Ø¯ØªØ±ÛŒÙ† Ø®Ø±ÛŒØ¯ Ø¹Ù…Ø±Ù… Ø¨ÙˆØ¯", "label": 0},  # Ù…Ù†ÙÛŒ    
    {"text": "Ù¾Ø³ Ø§Ø² Ø¯Ùˆ Ø±ÙˆØ² Ø®Ø±Ø§Ø¨ Ø´Ø¯", "label": 0},  # Ù…Ù†ÙÛŒ    
    {"text": "Ø§ØµÙ„Ø§ Ø¨Ù‡ Ø¯Ø±Ø¯ Ù†Ù…ÛŒ Ø®ÙˆØ±Ù‡", "label": 0},  # Ù…Ù†ÙÛŒ    
    {"text": "Ø­ÛŒÙ Ù¾ÙˆÙ„Ù… Ú©Ù‡ Ø®Ø±Ø¬ Ø§ÛŒÙ† Ú†ÛŒØ² Ø¨Ø¯Ù… Ú©Ø±Ø¯Ù…", "label": 0},  # Ù…Ù†ÙÛŒ   
    {"text": "Ø¨Ø¯ Ù‡Ø³Øª", "label": 0},  # Ù…Ù†ÙÛŒ    
    {"text": "Ø¨Ø¯ Ø®ÛŒÙ„ÛŒ", "label": 0},  # Ù…Ù†ÙÛŒ  
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ù†Ø«ÛŒ
    {"text": "Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø§Ø³Øª", "label": 2},  # Ø®Ù†Ø«ÛŒ
    {"text": "Ù†Ù‡ Ø®ÙˆØ¨Ù‡ Ù†Ù‡ Ø¨Ø¯", "label": 2},  # Ø®Ù†Ø«ÛŒ
    {"text": "Ù†Ù‡ Ø®ÙˆØ¨ Ù†Ù‡ Ø¨Ø¯", "label": 2},  # Ø®Ù†Ø«ÛŒ
    {"text": "Ù…Ø­ØµÙˆÙ„ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø³Øª", "label": 2},  # Ø®Ù†Ø«ÛŒ
    {"text": "Ú†ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ú¯ÙØªÙ† Ù†Ø¯Ø§Ø±Ù…", "label": 2},  # Ø®Ù†Ø«ÛŒ
    {"text": "Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ù…Ø«Ù„ Ø¨Ù‚ÛŒÙ‡ Ù…Ø­ØµÙˆÙ„Ø§Øª", "label": 2},  # Ø®Ù†Ø«ÛŒ
    {"text": "Ù†Ù‡ Ø¹Ø§Ù„ÛŒ Ù†Ù‡ Ø¨Ø¯", "label": 2},  # Ø®Ù†Ø«ÛŒ
    {"text": "Ù…Ù†Ø§Ø³Ø¨ Ù‚ÛŒÙ…ØªØ´ Ø¨ÙˆØ¯", "label": 2},  # Ø®Ù†Ø«ÛŒ
    {"text": "Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„", "label": 2},  # Ø®Ù†Ø«ÛŒ
    {"text": "Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ù‡Ø³Øª", "label": 2},  # Ø®Ù†Ø«ÛŒ
]

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§ÙØ±Ø§Ù…
df = pd.DataFrame(reviews_data)
print(fa("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡:"))
print(df.head())

# --------------------------------------------------------------------
# 2. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
# --------------------------------------------------------------------
def preprocess_persian_text(text):
    """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø¹Ù„Ø§Ø¦Ù… Ø®Ø§Øµ
    text = re.sub(r'[Û°-Û¹0-9]', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = re.sub(r'\s+', ' ', text)
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
    text = text.lower().strip()
    
    return text

# Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
df['cleaned_text'] = df['text'].apply(preprocess_persian_text)
print(fa("\nÙ…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€Œ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡:"))
print(df[['text', 'cleaned_text']].head())

# --------------------------------------------------------------------
# 3. Ø§ÛŒØ¬Ø§Ø¯ Vocabulary Ùˆ Tokenizer Ø³Ø§Ø¯Ù‡
# --------------------------------------------------------------------
def build_vocabulary(texts, vocab_size=1000):
    """Ø³Ø§Ø®Øª vocabulary Ø§Ø² Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ"""
    word_counts = {}
    
    for text in texts:
        words = text.split()
        for word in words:
            if len(word) > 2:  # ÙÙ‚Ø· Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø·ÙˆÙ„ Ø¨ÛŒØ´ØªØ± Ø§Ø² 2
                word_counts[word] = word_counts.get(word, 0) + 1
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÚ©Ø±Ø§Ø±
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    
    # Ø§ÛŒØ¬Ø§Ø¯ vocabulary
    vocab = {'<PAD>': 0, '<UNK>': 1}
    for i, (word, count) in enumerate(sorted_words[:vocab_size-2]):
        vocab[word] = i + 2
    
    return vocab

def text_to_sequence(text, vocab, max_length=20):
    """ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø¹Ø¯Ø§Ø¯"""
    words = text.split()
    sequence = []
    
    for word in words:
        sequence.append(vocab.get(word, vocab['<UNK>']))
    
    # padding ÛŒØ§ truncate
    if len(sequence) < max_length:
        sequence = sequence + [vocab['<PAD>']] * (max_length - len(sequence))
    else:
        sequence = sequence[:max_length]
    
    return sequence

# Ø³Ø§Ø®Øª vocabulary
vocab = build_vocabulary(df['cleaned_text'].tolist(), vocab_size=100)
print(fa(f"\nØ§Ù†Ø¯Ø§Ø²Ù‡ vocabulary: {len(vocab)}"))
print(fa("Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² vocabulary:"), dict(list(vocab.items())[:10]))

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙˆÙ† Ø¨Ù‡ sequences
max_length = 15
X_sequences = []
for text in df['cleaned_text']:
    seq = text_to_sequence(text, vocab, max_length)
    X_sequences.append(seq)

X = np.array(X_sequences)
y = np.array(df['label'])

print(fa(f"\nØ´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: X={X.shape}, y={y.shape}"))

# --------------------------------------------------------------------
# 4. ØªØ¹Ø±ÛŒÙ MultiHeadAttention Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡
# --------------------------------------------------------------------
class SimpleMultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(SimpleMultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.depth = d_model // num_heads
        
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        self.dense = tf.keras.layers.Dense(d_model)
        
    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def call(self, v, k, q):
        batch_size = tf.shape(q)[0]
        
        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)
        
        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)
        
        # Attention Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡
        matmul_qk = tf.matmul(q, k, transpose_b=True)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(tf.cast(self.depth, tf.float32))
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        
        output = tf.matmul(attention_weights, v)
        output = tf.transpose(output, perm=[0, 2, 1, 3])
        output = tf.reshape(output, (batch_size, -1, self.d_model))
        
        return self.dense(output)

# --------------------------------------------------------------------
# 5. ØªØ¹Ø±ÛŒÙ Transformer Block Ú©Ø§Ù…Ù„
# --------------------------------------------------------------------  
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(TransformerBlock, self).__init__()
        
        self.mha = SimpleMultiHeadAttention(d_model, num_heads)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(dff, activation='relu'),
            tf.keras.layers.Dense(d_model)
        ])
        
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
    
    def call(self, x, training):
        # Self-Attention
        attn_output = self.mha(x, x, x)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)
        
        # Feed Forward Network
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)
        
        return out2

# --------------------------------------------------------------------
# 6. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª (Ø³Ù‡ Ú©Ù„Ø§Ø³Ù‡)
# --------------------------------------------------------------------
def build_sentiment_model(vocab_size, max_length, d_model=64, num_heads=4, dff=128, num_classes=3):
    """Ø³Ø§Ø®Øª Ù…Ø¯Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Transformer"""
    
    inputs = tf.keras.Input(shape=(max_length,))
    
    # Embedding Layer
    embedding = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
    
    # Positional Encoding Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡
    positions = tf.range(start=0, limit=max_length, delta=1)
    positions = tf.expand_dims(positions, 0)
    positional_encoding = tf.keras.layers.Embedding(max_length, d_model)(positions)
    
    x = embedding + positional_encoding
    
    # Transformer Block
    transformer_block = TransformerBlock(d_model, num_heads, dff)
    x = transformer_block(x, training=True)
    
    # Global Average Pooling
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    
    # Classification Head (Ø§Ú©Ù†ÙˆÙ† Ø³Ù‡ Ú©Ù„Ø§Ø³Ù‡)
    x = tf.keras.layers.Dropout(0.2)(x)
    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)  # ØªØºÛŒÛŒØ± Ø¨Ù‡ softmax Ù†Ø³Ø¨Øª Ø¨Ù‡ Ú©Ø¯ SentimentAnalysisWithTransformer.py
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model
# def build_sentiment_model(vocab_size, max_length, d_model=32, num_heads=2, dff=64, num_classes=3):
#     """Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡â€ŒØªØ±"""
    
#     inputs = tf.keras.Input(shape=(max_length,))
    
#     # Embedding
#     embedding = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
    
#     # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Transformer Ø¨Ø§ LSTM Ø³Ø§Ø¯Ù‡â€ŒØªØ±
#     x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedding)
#     x = tf.keras.layers.Dropout(0.3)(x)
    
#     # Classification Head
#     outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
    
#     model = tf.keras.Model(inputs=inputs, outputs=outputs)
#     return model

# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø¨Ø§ Ø³Ù‡ Ú©Ù„Ø§Ø³
vocab_size = len(vocab)
num_classes = 3  # Ù…Ù†ÙÛŒØŒ Ù…Ø«Ø¨ØªØŒ Ø®Ù†Ø«ÛŒ
model = build_sentiment_model(vocab_size, max_length, num_classes=num_classes)

print(fa("\nØ®Ù„Ø§ØµÙ‡ Ù…Ø¯Ù„:"))
model.summary()

# Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„ (Ø§Ú©Ù†ÙˆÙ† Ø¨Ø§ loss Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡)
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',  # ØªØºÛŒÛŒØ± Ø¨Ù‡ Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø²ÛŒØ§Ù†
    metrics=['accuracy']
)

# --------------------------------------------------------------------
# 7. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
# --------------------------------------------------------------------
print(fa("\nğŸ”¨ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„..."))

# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ train Ùˆ test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 3. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆØ²Ù† Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¨Ù„Ù‡ Ø¨Ø§ Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„
class_weights = {0: 2.0, 1: 2.0, 2: 1.0}  # ÙˆØ²Ù† Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø§Ù‚Ù„ÛŒØª

history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=4,  # Ø¨Ú† Ø³Ø§ÛŒØ² Ú©ÙˆÚ†Ú©ØªØ±
    validation_split=0.2,
    class_weight=class_weights,  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆØ²Ù† Ú©Ù„Ø§Ø³
    verbose=1
)

# --------------------------------------------------------------------
# 8. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
# --------------------------------------------------------------------
print(fa("\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„..."))

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)  # Ú¯Ø±ÙØªÙ† Ú©Ù„Ø§Ø³ Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø§Ø­ØªÙ…Ø§Ù„

accuracy = accuracy_score(y_test, y_pred)
print(fa(f"Ø¯Ù‚Øª Ù…Ø¯Ù„: {accuracy:.2f}"))

print(fa("\nÚ¯Ø²Ø§Ø±Ø´ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:"))
# ØªØ¹Ø±ÛŒÙ Ù†Ø§Ù… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´
class_names = [fa('Ù…Ù†ÙÛŒ'), fa('Ù…Ø«Ø¨Øª'), fa('Ø®Ù†Ø«ÛŒ')]
print(classification_report(y_test, y_pred, target_names=class_names))

# --------------------------------------------------------------------
# 9. ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¬Ø¯ÛŒØ¯
# --------------------------------------------------------------------
print(fa("\nğŸ§ª ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¬Ø¯ÛŒØ¯:"))

test_sentences = [
    "Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ø¹Ø§Ù„ÛŒ Ø§Ø³Øª",  # Ù…Ø«Ø¨Øª
    "Ø®ÛŒÙ„ÛŒ Ø¨Ø¯ Ø¨ÙˆØ¯",         # Ù…Ù†ÙÛŒ
    "Ú©ÛŒÙÛŒØª Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ø¯",     # Ù…Ø«Ø¨Øª
    "Ù¾Ø´ÛŒÙ…Ø§Ù† Ø´Ø¯Ù…",          # Ù…Ù†ÙÛŒ
    "Ù…Ø­ØµÙˆÙ„ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø§Ø³Øª",    # Ø®Ù†Ø«ÛŒ
    "Ù†Ù‡ Ø®ÙˆØ¨Ù‡ Ù†Ù‡ Ø¨Ø¯",       # Ø®Ù†Ø«ÛŒ
    "Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¨ÙˆØ¯",       # Ø®Ù†Ø«ÛŒ
]

# ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø³Ù‡ Ú©Ù„Ø§Ø³
def predict_sentiment(text, model, vocab, max_length=15):
    # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
    cleaned = preprocess_persian_text(text)
    sequence = text_to_sequence(cleaned, vocab, max_length)
    sequence = np.array([sequence])
    
    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
    prediction_probs = model.predict(sequence)[0]
    predicted_class = np.argmax(prediction_probs)
    confidence = prediction_probs[predicted_class]
    
    # Ù†Ú¯Ø§Ø´Øª Ú©Ù„Ø§Ø³ Ø¨Ù‡ Ù†Ø§Ù… Ø§Ø­Ø³Ø§Ø³
    sentiment_map = {
        0: "Ù…Ù†ÙÛŒ ğŸ‘",
        1: "Ù…Ø«Ø¨Øª ğŸ‘", 
        2: "Ø®Ù†Ø«ÛŒ ğŸ˜"
    }
    
    return sentiment_map[predicted_class], confidence, prediction_probs

for sentence in test_sentences:
    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
    sentiment, confidence, probs = predict_sentiment(sentence, model, vocab, max_length)
    
    print(fa(f"Ø¬Ù…Ù„Ù‡: '{sentence}'"))
    print(fa(f"Ø§Ø­Ø³Ø§Ø³: {sentiment} (Ø§Ø¹ØªÙ…Ø§Ø¯: {confidence:.2f})"))
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² f-string Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ù…Ø´Ú©Ù„
    print(fa(f"ØªÙˆØ²ÛŒØ¹ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª: Ù…Ù†ÙÛŒ={probs[0]:.2f}, Ù…Ø«Ø¨Øª={probs[1]:.2f}, Ø®Ù†Ø«ÛŒ={probs[2]:.2f}"))
    print("-" * 50)

# --------------------------------------------------------------------
# 10. ØªØ¬Ø³Ù… Ù†ØªØ§ÛŒØ¬
# --------------------------------------------------------------------
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label=fa('Ø¯Ù‚Øª Ø¢Ù…ÙˆØ²Ø´'))
plt.plot(history.history['val_accuracy'], label=fa('Ø¯Ù‚Øª validation'))
plt.title(fa('Ø¯Ù‚Øª Ù…Ø¯Ù„'))
plt.xlabel(fa('Ø¯ÙˆØ±Ù‡'))
plt.ylabel(fa('Ø¯Ù‚Øª'))
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label=fa('Ø®Ø·Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´'))
plt.plot(history.history['val_loss'], label=fa('Ø®Ø·Ø§ÛŒ validation'))
plt.title(fa('Ø®Ø·Ø§ÛŒ Ù…Ø¯Ù„'))
plt.xlabel(fa('Ø¯ÙˆØ±Ù‡'))
plt.ylabel(fa('Ø®Ø·Ø§'))
plt.legend()

plt.tight_layout()
plt.show()

print(fa("\nğŸ‰ Ù…Ø¯Ù„ Transformer Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÙØ§Ø±Ø³ÛŒ (Ø³Ù‡ Ú©Ù„Ø§Ø³Ù‡) Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!"))