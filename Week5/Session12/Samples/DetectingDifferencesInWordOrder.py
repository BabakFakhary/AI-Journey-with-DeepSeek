#                                                                     ุจู ูุงู ุฎุฏุง
# install: pip install --upgrade arabic-reshaper
import arabic_reshaper
#-------------------------------------------------------
# install: pip install python-bidi
from bidi.algorithm import get_display
#-------------------------------------------------------
import torch  #  ฺฉุชุงุจุฎุงูู ุงุตู ุงุฏฺฏุฑ ุนูู
import torch.nn as nn
import numpy as np # ูุญุงุณุจุงุช ุนุฏุฏ
import matplotlib.pyplot as plt 
import math

# --------------------------------------------------------------------------------------------------------
# ุชุดุฎุต ุชูุงูุช ุชุฑุชุจ ฺฉููุงุช
# --------------------------------------------------------------------------------------------------------

# ุชูุธูุงุช ููุงุด ูุงุฑุณ
def fa(text):
    return get_display(arabic_reshaper.reshape(text))

# 1. ฺฉูุงุณ Positional Encoding
# ุงู ุงูฺฏู ุจู ูุฏู ฺฉูฺฉ ูโฺฉูุฏ ูู ูููุนุช ูุทูู ู ูู ูููุนุช ูุณุจ ฺฉููุงุช ุฑุง ุงุฏ ุจฺฏุฑุฏ
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        # ุงุฌุงุฏ ูุงุชุฑุณ positional encoding
        pe = torch.zeros(max_len, d_model)
        
        #  # ูุญุงุณุจู ูููุนุชโูุง (0 ุชุง max_len-1)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # ูุญุงุณุฏู divisor term
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        # ุงุนูุงู ุณููุณ ู ฺฉุณููุณ
          # ุงู ฺฉูุงุณ ุงุฒ ูุฑููู ุณููุณ ู ฺฉุณููุณ ุงุณุชูุงุฏู ูโฺฉูุฏ ุชุง ุจู ูุฑ ูููุนุช ุฏุฑ ุฌููู ฺฉ ุงูุถุง ุนุฏุฏ ููุญุตุฑ ุจู ูุฑุฏ ุจุฏูุฏ.
        pe[:, 0::2] = torch.sin(position * div_term)  # ุจุฑุง ูููุนุชโูุง ุฒูุฌ ุงุฒ ุณููุณ ุงุณุชูุงุฏู ูโฺฉูุฏ
        pe[:, 1::2] = torch.cos(position * div_term)  # ุจุฑุง ูููุนุชโูุง ูุฑุฏ ุงุฒ ฺฉุณููุณ ุงุณุชูุงุฏู ูโฺฉูุฏ
        
        # ุงุถุงูู ฺฉุฑุฏู ุจุนุฏ batch
        pe = pe.unsqueeze(0)
        
        # ุซุจุช ุจู ุนููุงู buffer (ูู parameter)
        self.register_buffer('pe', pe)
    
    # ุงู ุชุงุจุน ุงุทูุงุนุงุช ูููุนุช ุฑุง ุจู ุจุฑุฏุงุฑูุง ฺฉููุงุช ุงุถุงูู ูโฺฉูุฏ
    def forward(self, x):
        # ุงุถุงูู ฺฉุฑุฏู positional encoding ุจู embedding
        return x + self.pe[:, :x.size(1)]

# 2. ุงุฌุงุฏ ูุฏู ุณุงุฏู
class SimpleModel(nn.Module):
    def __init__(self, vocab_size, d_model):
        super(SimpleModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        
    def forward(self, x):
        x = self.embedding(x)
        x = self.pos_encoder(x)
        return x

# 3. ุชุณุช ุนูู
def test_real_example():
    print(fa("๐ฏ ูุซุงู ุนูู: ุชุดุฎุต ุชูุงูุช ุชุฑุชุจ ฺฉููุงุช"))
    print(fa("=" * 50))
    
    # ุงุฌุงุฏ ูุฏู
    vocab_size = 10  # 10 ฺฉููู ูุฎุชูู
    d_model = 8     # ุงุจุนุงุฏ ฺฉูฺฺฉ ุจุฑุง ุณุงุฏฺฏ
    model = SimpleModel(vocab_size, d_model)
    
    # ุฏู ุฌููู ุจุง ฺฉููุงุช ฺฉุณุงู ุงูุง ุชุฑุชุจ ูุฎุชูู
    # ูุฑุถ ฺฉูุฏ: 1=ฺฏุฑุจู, 2=ุณฺฏ, 3=ููุด, 4=ูพุฑุฏ
    sentence1 = torch.tensor([[1, 2, 3, 4]])  # "ฺฏุฑุจู ุณฺฏ ููุด ูพุฑุฏ"
    sentence2 = torch.tensor([[4, 3, 2, 1]])  # "ูพุฑุฏ ููุด ุณฺฏ ฺฏุฑุจู"
    
    print(fa("ุฌููู ฑ: ฺฏุฑุจู ุณฺฏ ููุด ูพุฑุฏ"))
    print(fa("ุฌููู ฒ: ูพุฑุฏ ููุด ุณฺฏ ฺฏุฑุจู"))
    print(fa("ฺฉุฏูุง ุฌููู ฑ:"), sentence1.tolist())
    print(fa("ฺฉุฏูุง ุฌููู ฒ:"), sentence2.tolist())
    print()
    
    # ูพุฑุฏุงุฒุด ุจุฏูู Positional Encoding
     # ุนู: ููุท ุงุฒ ูุณูุช embedding ูุฏู ุงุณุชูุงุฏู ฺฉู
    embeddings_only = model.embedding(sentence1)
    print(fa("๐ ุจุฏูู Positional Encoding:"))
    print(fa("ุดฺฉู embeddings:"), embeddings_only.shape)
    print(fa("ููุงุฏุฑ ููููู (ุงููู ฺฉููู):"))
    print(embeddings_only[0, 0, :4].detach().numpy())  # ููุท 4 ุจุนุฏ ุงูู
    print()
    
    # ูพุฑุฏุงุฒุด ุจุง Positional Encoding
     # ุนู: ุงุฒ ฺฉู ูุฏู ุงุณุชูุงุฏู ฺฉู (ูู embedding + ูู positional encoding)
    with_pos_encoding = model(sentence1)
    print(fa("๐ ุจุง Positional Encoding:"))
    print(fa("ุดฺฉู ุฎุฑูุฌ:"), with_pos_encoding.shape)
    print(fa("ููุงุฏุฑ ููููู (ุงููู ฺฉููู):"))
    print(with_pos_encoding[0, 0, :4].detach().numpy())  # ููุท 4 ุจุนุฏ ุงูู
    print()

# 4. ูุญุงุณุจู ุดุจุงูุช ุจู ุฏู ุฌููู
    def calculate_similarity():
        # ูพุฑุฏุงุฒุด ูุฑ ุฏู ุฌููู
        emb1 = model(sentence1)
        emb2 = model(sentence2)
        
        # ูุญุงุณุจู ุดุจุงูุช ฺฉุณููุณ
          # ฺฉ ุชุงุจุน ุจุณุงุฑ ููู ู ฺฉุงุฑุจุฑุฏ ุงุณุช ฺฉู ูุฒุงู ุดุจุงูุช ุจู ุฏู ุจุฑุฏุงุฑ ุฑุง ูุญุงุณุจู ูโฺฉูุฏ
          # ุงู ุชุงุจุน ฺฉุณููุณ ุฒุงูู ุจู ุฏู ุจุฑุฏุงุฑ ุฑุง ูุญุงุณุจู ูโฺฉูุฏ. ูุฑ ฺู ุงู ููุฏุงุฑ ุจู ฑ ูุฒุฏฺฉโุชุฑ ุจุงุดุฏุ ุฏู ุจุฑุฏุงุฑ ุดุจูโุชุฑ ูุณุชูุฏ
        similarity = torch.cosine_similarity(
            emb1.flatten(), 
            emb2.flatten(), 
            dim=0
        )
        
        return similarity.item()
    
# 5. ุชุณุช ฺูุฏุจุงุฑู ุจุฑุง ุฏุฑฺฉ ุจูุชุฑ
    print(fa("๐ ููุงุณู ุฏู ุฌููู:"))
    for i in range(3):
        similarity = calculate_similarity()
        print(fa(f"ุดุจุงูุช ุจู ุฏู ุฌููู: {similarity:.4f}"))
    
    print()
    print(fa("โ ูุชุฌู: ูุฑ ฺู ุดุจุงูุช ฺฉูุชุฑ ุจุงุดุฏุ ูุฏู ุจูุชุฑ ุชูุงูุช ุชุฑุชุจ ุฑุง ุชุดุฎุต ูโุฏูุฏ!"))

# 6. ุชุฌุณู Positional Encoding ุจุฑุง ุฏุฑฺฉ ุจูุชุฑ
# ุงู ุชุงุจุน ูโุฎูุงูุฏ ูุดุงู ุฏูุฏ ฺฉู ฺฺฏููู ูุฑ ูููุนุช ุฏุฑ ุฌููู ฺฉ ุงูุถุง ุนุฏุฏ ููุญุตุฑ ุจู ูุฑุฏ ุฏุงุฑุฏ
def visualize_simple_pe():
    print(fa("\n๐ ุชุฌุณู Positional Encoding ุจุฑุง 5 ูููุนุช ู 4 ุจุนุฏ:"))
    
    # ุงุฌุงุฏ PE ุจุฑุง ุงุจุนุงุฏ ฺฉูฺฺฉ
     # ต ูููุนุช: ุนู ต ฺฉููู ุฏุฑ ุฌููู
     # ด ุจุนุฏ: ุนู ูุฑ ฺฉููู ุจุง ด ุนุฏุฏ ููุงุด ุฏุงุฏู ูโุดูุฏ (ุณุงุฏู ุดุฏู)
    pe = torch.zeros(5, 4)  # 5 ูููุนุชุ 4 ุจุนุฏ
    position = torch.arange(0, 5, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, 4, 2).float() * (-math.log(10000.0) / 4))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    # ุฎุฑูุฌ ูุงุชุฑุณ
    
      # ุณุทุฑูุง: ูููุนุชโูุง ฺฉููุงุช (0 ุชุง 4)
      # ุณุชููโูุง: ุงุจุนุงุฏ ูุฎุชูู (0 ุชุง 3)
      # ููุงุฏุฑ: ุจู -1 ุชุง 1
  
      # ุจุฑุง ูููุนุช ฐ (ุงููู ฺฉููู):
      #[ 0.00,  1.00,  0.00,  1.00]  # ุจุณุงุฑ ุณุงุฏู
  
      # ุจุฑุง ูููุนุช ฒ (ุณููู ฺฉููู):  
      # [ 0.91, -0.42,  0.0002, 0.999998]  # ูพฺุฏูโุชุฑ
      
      # ุจุฑุง ูููุนุช ด (ูพูุฌูู ฺฉููู):
      # [-0.76, -0.65,  0.0004, 0.999992]  # ฺฉุงููุงู ูุชูุงูุช

    print(fa("ูุงุชุฑุณ Positional Encoding (5 ูููุนุช ร 4 ุจุนุฏ):"))
    print(pe.numpy())
    print()
    
    # ุฑุณู ุณุงุฏู
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.imshow(pe.numpy(), cmap='viridis', aspect='auto')
    plt.title(fa('Heatmap Positional Encoding'))
    plt.xlabel(fa('ุจุนุฏ'))
    plt.ylabel(fa('ูููุนุช'))
    plt.colorbar()
    
    plt.subplot(1, 2, 2)
    for i in range(4):
        plt.plot(pe[:, i].numpy(), label=fa(f'ุจุนุฏ {i}'))
    plt.title(fa('ููุงุฏุฑ ุจุฑุง ูุฑ ุจุนุฏ'))
    plt.xlabel(fa('ูููุนุช'))
    plt.ylabel(fa('ููุฏุงุฑ'))
    plt.legend()
    
    plt.tight_layout()
    plt.show()

# 7. ุงุฌุฑุง ูุซุงู
if __name__ == "__main__":
    test_real_example()
    visualize_simple_pe()
    
    print(fa("\n" + "=" * 50))
    print(fa("๐ ุฎูุงุตู ููููู:"))
    print(fa("โข ุจุฏูู Positional Encoding: ููู ุฌููุงุช ุดุจู ูู ุฏุฏู ูโุดููุฏ"))
    print(fa("โข ุจุง Positional Encoding: ุชุฑุชุจ ฺฉููุงุช ุชุดุฎุต ุฏุงุฏู ูโุดูุฏ"))
    print(fa("โข ูุฑ ูููุนุช ุงูุถุง ุนุฏุฏ ููุญุตุฑ ุจู ูุฑุฏ ุฏุงุฑุฏ"))
    print(fa("โข ูุฏู ูโูููุฏ 'ฺฏุฑุจู ุณฺฏ ุฑุง ูโุฒูุฏ' โ 'ุณฺฏ ฺฏุฑุจู ุฑุง ูโุฒูุฏ'"))