#                                                                  Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§
# install: pip install --upgrade arabic-reshaper
import arabic_reshaper
#-------------------------------------------------------
# install: pip install python-bidi
from bidi.algorithm import get_display
#-------------------------------------------------------
import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import re
import json
import matplotlib.pyplot as plt

# ---------------------------------------------------------------------------------------------------
# ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª
# ---------------------------------------------------------------------------------------------------

def fa(text):
    return get_display(arabic_reshaper.reshape(text)) 

# --------------------------------------------------------------------
# 1. ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙØ§Ø±Ø³ÛŒ (Ù†Ø¸Ø±Ø§Øª Ù…Ø­ØµÙˆÙ„Ø§Øª)
# --------------------------------------------------------------------
# Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù†Ø¸Ø±Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒ ÙØ§Ø±Ø³ÛŒ
reviews_data = [
    {"text": "Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ ÙˆØ§Ù‚Ø¹Ø§ Ø¹Ø§Ù„ÛŒ Ø§Ø³Øª Ùˆ Ú©ÛŒÙÛŒØª ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ Ø§ÛŒ Ø¯Ø§Ø±Ø¯", "label": 1},
    {"text": "Ø®ÛŒÙ„ÛŒ Ø¨Ø¯ Ùˆ Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø¨ÙˆØ¯ØŒ Ù¾Ø´ÛŒÙ…Ø§Ù† Ø´Ø¯Ù…", "label": 0},
    {"text": "Ù‚ÛŒÙ…Øª Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¯Ø§Ø±Ø¯ Ùˆ Ø§Ø±Ø²Ø´ Ø®Ø±ÛŒØ¯ Ø¯Ø§Ø±Ø¯", "label": 1},
    {"text": "Ø§ØµÙ„Ø§ ØªÙˆØµÛŒÙ‡ Ù†Ù…ÛŒ Ú©Ù†Ù…ØŒ waste of money", "label": 0},
    {"text": "Ø³Ø±ÛŒØ¹ Ø±Ø³ÛŒØ¯ Ùˆ Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯", "label": 1},
    {"text": "Ù…Ø­ØµÙˆÙ„ Ù…Ø¹ÛŒÙˆØ¨ Ø±Ø³ÛŒØ¯ØŒ Ø¨Ø³ÛŒØ§Ø± Ù†Ø§Ø±Ø§Ø­ØªÙ…", "label": 0},
    {"text": "Ú©ÛŒÙÛŒØª Ø³Ø§Ø®Øª Ø¹Ø§Ù„ÛŒØŒ Ú©Ø§Ù…Ù„Ø§ Ø±Ø§Ø¶ÛŒ Ù‡Ø³ØªÙ…", "label": 1},
    {"text": "Ø¨Ø¯ØªØ±ÛŒÙ† Ø®Ø±ÛŒØ¯ Ø¹Ù…Ø±Ù… Ø¨ÙˆØ¯", "label": 0},
    {"text": "Ú©Ø§Ø±Ø§ÛŒÛŒ ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡ØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒ Ú©Ù†Ù…", "label": 1},
    {"text": "Ù¾Ø³ Ø§Ø² Ø¯Ùˆ Ø±ÙˆØ² Ø®Ø±Ø§Ø¨ Ø´Ø¯", "label": 0},
    {"text": "Ø·Ø±Ø§Ø­ÛŒ Ø²ÛŒØ¨Ø§ Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¹Ø§Ù„ÛŒ", "label": 1},
    {"text": "Ø§ØµÙ„Ø§ Ø¨Ù‡ Ø¯Ø±Ø¯ Ù†Ù…ÛŒ Ø®ÙˆØ±Ù‡", "label": 0},
    {"text": "Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‚ÛŒÙ…ØªØ´ Ø¹Ø§Ù„ÛŒÙ‡", "label": 1},
    {"text": "Ø­ÛŒÙ Ù¾ÙˆÙ„Ù… Ú©Ù‡ Ø®Ø±Ø¬ Ø§ÛŒÙ† Ú†ÛŒØ² Ø¨Ø¯Ù… Ú©Ø±Ø¯Ù…", "label": 0},
    {"text": "Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ÛŒæ‰¿è¯º Ø´Ø¯Ù‡ Ø±Ùˆ Ø¯Ø§Ø±Ù‡", "label": 1},
]

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§ÙØ±Ø§Ù…
df = pd.DataFrame(reviews_data)
print(fa("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡:"))
print(df.head())

# --------------------------------------------------------------------
# 2. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
# --------------------------------------------------------------------
def preprocess_persian_text(text):
    """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø¹Ù„Ø§Ø¦Ù… Ø®Ø§Øµ
    text = re.sub(r'[Û°-Û¹0-9]', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = re.sub(r'\s+', ' ', text)
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
    text = text.lower().strip()
    
    return text

# Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
df['cleaned_text'] = df['text'].apply(preprocess_persian_text)
print(fa("\nÙ…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€Œ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡:"))
print(df[['text', 'cleaned_text']].head())

# --------------------------------------------------------------------
# 3. Ø§ÛŒØ¬Ø§Ø¯ Vocabulary Ùˆ Tokenizer Ø³Ø§Ø¯Ù‡
# --------------------------------------------------------------------
def build_vocabulary(texts, vocab_size=1000):
    """Ø³Ø§Ø®Øª vocabulary Ø§Ø² Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ"""
    word_counts = {}
    
    for text in texts:
        words = text.split()
        for word in words:
            if len(word) > 2:  # ÙÙ‚Ø· Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø·ÙˆÙ„ Ø¨ÛŒØ´ØªØ± Ø§Ø² 2
                word_counts[word] = word_counts.get(word, 0) + 1
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÚ©Ø±Ø§Ø±
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    
    # Ø§ÛŒØ¬Ø§Ø¯ vocabulary
    vocab = {'<PAD>': 0, '<UNK>': 1}
    for i, (word, count) in enumerate(sorted_words[:vocab_size-2]):
        vocab[word] = i + 2
    
    return vocab

def text_to_sequence(text, vocab, max_length=20):
    """ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø¹Ø¯Ø§Ø¯"""
    words = text.split()
    sequence = []
    
    for word in words:
        sequence.append(vocab.get(word, vocab['<UNK>']))
    
    # padding ÛŒØ§ truncate
    if len(sequence) < max_length:
        sequence = sequence + [vocab['<PAD>']] * (max_length - len(sequence))
    else:
        sequence = sequence[:max_length]
    
    return sequence

# Ø³Ø§Ø®Øª vocabulary
vocab = build_vocabulary(df['cleaned_text'].tolist(), vocab_size=100)
print(fa(f"\nØ§Ù†Ø¯Ø§Ø²Ù‡ vocabulary: {len(vocab)}"))
print(fa("Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² vocabulary:"), dict(list(vocab.items())[:10]))

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙˆÙ† Ø¨Ù‡ sequences
max_length = 15
X_sequences = []
for text in df['cleaned_text']:
    seq = text_to_sequence(text, vocab, max_length)
    X_sequences.append(seq)

X = np.array(X_sequences)
y = np.array(df['label'])

print(fa(f"\nØ´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: X={X.shape}, y={y.shape}"))

# --------------------------------------------------------------------
# 4. ØªØ¹Ø±ÛŒÙ MultiHeadAttention Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡
# --------------------------------------------------------------------
  #  Self-Attention (ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø®ÙˆØ¯) : Ú©Ø§Ø±ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯: Ø¨Ù‡ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯: "Ø¨Ù‡ Ù‡Ù…Ù‡ Ú©Ù„Ù…Ø§Øª Ø¯ÛŒÚ¯Ø± Ù†Ú¯Ø§Ù‡ Ú©Ù† Ùˆ Ø¨Ø¨ÛŒÙ† Ú†Ù‚Ø¯Ø± Ø¨Ù‡ Ù‡Ø±Ú©Ø¯Ø§Ù… Ø¨Ø§ÛŒØ¯ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒ
class SimpleMultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(SimpleMultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.depth = d_model // num_heads
        
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        self.dense = tf.keras.layers.Dense(d_model)
        
    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def call(self, v, k, q):
        batch_size = tf.shape(q)[0]
        
        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)
        
        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)
        
        # Attention Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡
        matmul_qk = tf.matmul(q, k, transpose_b=True)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(tf.cast(self.depth, tf.float32))
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        
        output = tf.matmul(attention_weights, v)
        output = tf.transpose(output, perm=[0, 2, 1, 3])
        output = tf.reshape(output, (batch_size, -1, self.d_model))
        
        return self.dense(output)

# --------------------------------------------------------------------
# 5. ØªØ¹Ø±ÛŒÙ Transformer Block Ú©Ø§Ù…Ù„
# --------------------------------------------------------------------  
class TransformerBlock(tf.keras.layers.Layer):
    # Transformer Block: 
      # Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ† Ù‡Ù…Ù‡ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
      # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ ØªØ±Ú©ÛŒØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ 
      # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¹Ù…ÛŒÙ‚ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
      # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ ØºÙ†ÛŒâ€ŒØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ 
      # Ø­ÙØ¸ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ØµÙ„ÛŒ + Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø¯ÛŒØ¯
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(TransformerBlock, self).__init__()
        
        #  Self-Attention (ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø®ÙˆØ¯) : Ú©Ø§Ø±ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯: Ø¨Ù‡ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯: "Ø¨Ù‡ Ù‡Ù…Ù‡ Ú©Ù„Ù…Ø§Øª Ø¯ÛŒÚ¯Ø± Ù†Ú¯Ø§Ù‡ Ú©Ù† Ùˆ Ø¨Ø¨ÛŒÙ† Ú†Ù‚Ø¯Ø± Ø¨Ù‡ Ù‡Ø±Ú©Ø¯Ø§Ù… Ø¨Ø§ÛŒØ¯ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒ
        self.mha = SimpleMultiHeadAttention(d_model, num_heads)

        # Feed Forward Network (Ø´Ø¨Ú©Ù‡ Ù¾ÛŒØ´â€ŒØ®ÙˆØ±) : Ú©Ø§Ø±ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯: "ÛŒÚ© Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø¶Ø§ÙÛŒ Ùˆ ØºÛŒØ±Ø®Ø·ÛŒ Ø±ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡"
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(dff, activation='relu'),
            tf.keras.layers.Dense(d_model)
        ])
        
        # Residual Connection Ùˆ Layer Norm  : Ú©Ø§Ø±ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯: "Ø¨Ø§Ø² Ù‡Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ ØªØ±Ú©ÛŒØ¨ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ú©Ù†"
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
    
    def call(self, x, training):
        # Self-Attention
        attn_output = self.mha(x, x, x)  # Q, K, V Ù‡Ù…Ù‡ x Ù‡Ø³ØªÙ†Ø¯ (Self-Attention)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)  # Residual connection
        
        # Feed Forward Network
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection
        
        return out2

# --------------------------------------------------------------------
# 6. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª
# --------------------------------------------------------------------
def build_sentiment_model(vocab_size, max_length, d_model=64, num_heads=4, dff=128):
    """Ø³Ø§Ø®Øª Ù…Ø¯Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ Transformer"""
    
    inputs = tf.keras.Input(shape=(max_length,))
    
    # Embedding Layer
    embedding = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
    
    # Positional Encoding Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡
    positions = tf.range(start=0, limit=max_length, delta=1)
    positions = tf.expand_dims(positions, 0)
    positional_encoding = tf.keras.layers.Embedding(max_length, d_model)(positions)
    
    x = embedding + positional_encoding
    
    # Transformer Block
    transformer_block = TransformerBlock(d_model, num_heads, dff)
    x = transformer_block(x, training=True)
    
    # Global Average Pooling
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    
    # Classification Head
    x = tf.keras.layers.Dropout(0.2)(x)
    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model

# Ø³Ø§Ø®Øª Ù…Ø¯Ù„
vocab_size = len(vocab)
model = build_sentiment_model(vocab_size, max_length)

print(fa("\nØ®Ù„Ø§ØµÙ‡ Ù…Ø¯Ù„:"))
model.summary()

# Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# --------------------------------------------------------------------
# 7. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
# --------------------------------------------------------------------
print(fa("\nğŸ”¨ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„..."))

# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ train Ùˆ test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=8,
    validation_split=0.2,
    verbose=1
)

# --------------------------------------------------------------------
# 8. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
# --------------------------------------------------------------------
print(fa("\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„..."))

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª
y_pred = model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred_binary)
print(fa(f"Ø¯Ù‚Øª Ù…Ø¯Ù„: {accuracy:.2f}"))

print(fa("\nÚ¯Ø²Ø§Ø±Ø´ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:"))
print(classification_report(y_test, y_pred_binary))

# --------------------------------------------------------------------
# 9. ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¬Ø¯ÛŒØ¯
# --------------------------------------------------------------------
print(fa("\nğŸ§ª ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¬Ø¯ÛŒØ¯:"))

test_sentences = [
    "Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ø¹Ø§Ù„ÛŒ Ø§Ø³Øª",  # Ù…Ø«Ø¨Øª
    "Ø®ÛŒÙ„ÛŒ Ø¨Ø¯ Ø¨ÙˆØ¯",         # Ù…Ù†ÙÛŒ
    "Ú©ÛŒÙÛŒØª Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ø¯",     # Ù…Ø«Ø¨Øª
    "Ù¾Ø´ÛŒÙ…Ø§Ù† Ø´Ø¯Ù…",          # Ù…Ù†ÙÛŒ
]

for sentence in test_sentences:
    # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
    cleaned = preprocess_persian_text(sentence)
    sequence = text_to_sequence(cleaned, vocab, max_length)
    sequence = np.array([sequence])
    
    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
    prediction = model.predict(sequence)[0][0]
    sentiment = "Ù…Ø«Ø¨Øª ğŸ‘" if prediction > 0.5 else "Ù…Ù†ÙÛŒ ğŸ‘"
    
    print(fa(f"Ø¬Ù…Ù„Ù‡: '{sentence}'"))
    print(fa(f"Ø§Ø­Ø³Ø§Ø³: {sentiment} (Ø§Ø¹ØªÙ…Ø§Ø¯: {prediction:.2f})"))
    print("-" * 40)

# --------------------------------------------------------------------
# 10. ØªØ¬Ø³Ù… Ù†ØªØ§ÛŒØ¬
# --------------------------------------------------------------------
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label=fa('Ø¯Ù‚Øª Ø¢Ù…ÙˆØ²Ø´'))
plt.plot(history.history['val_accuracy'], label=fa('Ø¯Ù‚Øª validation'))
plt.title(fa('Ø¯Ù‚Øª Ù…Ø¯Ù„'))
plt.xlabel(fa('Ø¯ÙˆØ±Ù‡'))
plt.ylabel(fa('Ø¯Ù‚Øª'))
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label=fa('Ø®Ø·Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´'))
plt.plot(history.history['val_loss'], label=fa('Ø®Ø·Ø§ÛŒ validation'))
plt.title(fa('Ø®Ø·Ø§ÛŒ Ù…Ø¯Ù„'))
plt.xlabel(fa('Ø¯ÙˆØ±Ù‡'))
plt.ylabel(fa('Ø®Ø·Ø§'))
plt.legend()

plt.tight_layout()
plt.show()

print(fa("\nğŸ‰ Ù…Ø¯Ù„ Transformer Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!"))