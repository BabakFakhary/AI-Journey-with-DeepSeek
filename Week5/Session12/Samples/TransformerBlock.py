#                                                                          Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§
# install: pip install --upgrade arabic-reshaper
import arabic_reshaper
#-------------------------------------------------------
# install: pip install python-bidi
from bidi.algorithm import get_display
#-------------------------------------------------------
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt 

# --------------------------------------------------------------------------------------------------------
# Transformer Block
# Input â†’ Multi-Head Attention â†’ Add & Norm â†’ Feed Forward â†’ Add & Norm â†’ Output
# Multi-Head Attention: Ø¯Ø±Ú© Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª
# Feed Forward Network (FFN): Ù¾Ø±Ø¯Ø§Ø²Ø´ ØºÛŒØ±Ø®Ø·ÛŒ
# Residual Connection: Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² vanishing gradient
# Layer Normalization: Ù¾Ø§ÛŒØ¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´
# --------------------------------------------------------------------------------------------------------

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´ ÙØ§Ø±Ø³ÛŒ
def fa(text):
  return get_display(arabic_reshaper.reshape(text))

# =============================================================================
# ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ residual_connection
# =============================================================================
def residual_connection(x, sublayer_output):
    return x + sublayer_output

# =============================================================================
# ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ LayerNormalization
# =============================================================================
class LayerNormalization:
    def __init__(self, d_model, eps=1e-6):
        self.gamma = np.ones((1, 1, d_model))
        self.beta = np.zeros((1, 1, d_model))
        self.eps = eps
        
    def forward(self, x):
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ùˆ ÙˆØ§Ø±ÛŒØ§Ù†Ø³
        mean = np.mean(x, axis=-1, keepdims=True)
        variance = np.var(x, axis=-1, keepdims=True)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        x_normalized = (x - mean) / np.sqrt(variance + self.eps)
        
        # Ù…Ù‚ÛŒØ§Ø³ Ùˆ Ø§Ù†ØªÙ‚Ø§Ù„
        return self.gamma * x_normalized + self.beta
    
# ØªØ§Ø¨Ø¹ tokenization Ø³Ø§Ø¯Ù‡
def simple_tokenizer(text, vocab):
    tokens = text.lower().split()
    return [vocab.get(token, vocab.get('<UNK>', 0)) for token in tokens]

# Ø§ÛŒØ¬Ø§Ø¯ vocabulary Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
vocab = {'the': 1, 'cat': 2, 'chased': 3, 'mouse': 4, 'dog': 5, 'runs': 6, 
         'sleeps': 7, 'eats': 8, 'cheese': 9, 'bird': 10, 'sings': 11, 
         'beautifully': 12, 'a': 13, '<UNK>': 0}

reverse_vocab = {v: k for k, v in vocab.items()}

# =============================================================================
# ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ MultiHeadAttention
# =============================================================================
class MultiHeadAttention:
    def __init__(self, d_model, num_heads):
        self.d_model = d_model
        self.num_heads = num_heads
        self.depth = d_model // num_heads
        
        # Ù…Ø§ØªØ±ÛŒØ³â€ŒÙ‡Ø§ÛŒ ÙˆØ²Ù†
        self.WQ = np.random.randn(d_model, d_model) * 0.01
        self.WK = np.random.randn(d_model, d_model) * 0.01
        self.WV = np.random.randn(d_model, d_model) * 0.01
        self.WO = np.random.randn(d_model, d_model) * 0.01
        
    def split_heads(self, x, batch_size):
        x = x.reshape(batch_size, -1, self.num_heads, self.depth)
        return x.transpose(0, 2, 1, 3)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        matmul_qk = np.matmul(Q, K.transpose(0, 1, 3, 2))
        dk = K.shape[-1]
        scaled_attention_logits = matmul_qk / np.sqrt(dk)
        
        if mask is not None:
            scaled_attention_logits += (mask * -1e9)
        
        attention_weights = self.softmax(scaled_attention_logits, axis=-1)
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def softmax(self, x, axis=-1):
        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)
    
    def call(self, x, mask=None):
        batch_size = x.shape[0]
        
        # ØªÙˆÙ„ÛŒØ¯ Q, K, V
        Q = np.dot(x, self.WQ)
        K = np.dot(x, self.WK)
        V = np.dot(x, self.WV)
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ heads
        Q = self.split_heads(Q, batch_size)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ attention
        scaled_attention, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # ØªØ±Ú©ÛŒØ¨ heads
        scaled_attention = scaled_attention.transpose(0, 2, 1, 3)
        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)
        
        # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ
        output = np.dot(concat_attention, self.WO)
        
        return output, attention_weights

# =============================================================================
# ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ FeedForwardNetwork
# =============================================================================
class FeedForwardNetwork:
    def __init__(self, d_model, d_ff):
        self.d_model = d_model
        self.d_ff = d_ff
        
        # Ù…Ø§ØªØ±ÛŒØ³â€ŒÙ‡Ø§ÛŒ ÙˆØ²Ù†
        self.W1 = np.random.randn(d_model, d_ff) * 0.01
        self.b1 = np.zeros((1, 1, d_ff))
        self.W2 = np.random.randn(d_ff, d_model) * 0.01
        self.b2 = np.zeros((1, 1, d_model))
        
    def relu(self, x):
        return np.maximum(0, x)
    
    def call(self, x):
        # Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„
        hidden = np.dot(x, self.W1) + self.b1
        hidden = self.relu(hidden)
        
        # Ù„Ø§ÛŒÙ‡ Ø¯ÙˆÙ…
        output = np.dot(hidden, self.W2) + self.b2
        
        return output

# =============================================================================
# ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ TransformerBlock
# =============================================================================
class TransformerBlock:
    def __init__(self, d_model, num_heads, d_ff):
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = FeedForwardNetwork(d_model, d_ff)
        self.ln1 = LayerNormalization(d_model)
        self.ln2 = LayerNormalization(d_model)
        
    def call(self, x, mask=None):
        # Multi-Head Attention
        attn_output, attn_weights = self.mha.call(x, mask)
        
        # Residual Connection + Layer Norm
        x1 = residual_connection(x, attn_output)
        x1 = self.ln1.forward(x1)
        
        # Feed Forward Network
        ffn_output = self.ffn.call(x1)
        
        # Residual Connection + Layer Norm
        x2 = residual_connection(x1, ffn_output)
        output = self.ln2.forward(x2)
        
        return output, attn_weights

# =============================================================================
# ØªØ³Øª Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ
# =============================================================================
print(fa("ğŸ”§ Ø´Ø±ÙˆØ¹ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡..."))

# ØªØ³Øª Multi-Head Attention
d_model = 64
num_heads = 4
batch_size = 1
seq_len = 5

mha = MultiHeadAttention(d_model, num_heads)
x = np.random.randn(batch_size, seq_len, d_model)

output, attention_weights = mha.call(x)
print("\nâœ… Multi-Head Attention Tested")
print("Input shape:", x.shape)
print("Output shape:", output.shape)
print("Attention weights shape:", attention_weights.shape)

# ØªØ³Øª FFN
d_ff = 256
ffn = FeedForwardNetwork(d_model, d_ff)
x = np.random.randn(batch_size, seq_len, d_model)
output = ffn.call(x)
print("\nâœ… FeedForwardNetwork Tested")
print("Input shape:", x.shape)
print("Output shape:", output.shape)

# ØªØ³Øª Residual Connection
original = np.random.randn(2, 3, 4)
sublayer_output = np.random.randn(2, 3, 4) * 0.1
result = residual_connection(original, sublayer_output)
print("\nâœ… Residual Connection Tested")
print("Original shape:", original.shape)
print("Sublayer output shape:", sublayer_output.shape)
print("Result shape:", result.shape)

# ØªØ³Øª Transformer Block
transformer_block = TransformerBlock(d_model, num_heads, d_ff)
x = np.random.randn(batch_size, seq_len, d_model)
output, attn_weights = transformer_block.call(x)
print("\nâœ… Transformer Block Tested")
print("Input shape:", x.shape)
print("Output shape:", output.shape)
print("Attention weights shape:", attn_weights.shape)

# =============================================================================
# Ø§ÛŒØ¬Ø§Ø¯ embedding layer Ùˆ positional encoding Ø³Ø§Ø¯Ù‡
# =============================================================================
vocab_size = len(vocab)
embedding_dim = d_model

# Ø§ÛŒØ¬Ø§Ø¯ embedding layer Ø³Ø§Ø¯Ù‡
embedding_layer = np.random.randn(vocab_size, embedding_dim) * 0.1

# Ø§ÛŒØ¬Ø§Ø¯ positional encoding Ø³Ø§Ø¯Ù‡
max_seq_len = 20
pe = np.zeros((max_seq_len, embedding_dim))
for pos in range(max_seq_len):
    for i in range(0, embedding_dim, 2):
        pe[pos, i] = np.sin(pos / (10000 ** (2 * i / embedding_dim)))
        if i + 1 < embedding_dim:
            pe[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / embedding_dim)))

# =============================================================================
# Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
# =============================================================================
def process_sentence_with_transformer(sentence, vocab, embedding_layer, pe, transformer_block):
    # Tokenization Ùˆ Embedding
    tokens = simple_tokenizer(sentence, vocab)
    if not tokens:
        return None
        
    word_embeddings = embedding_layer[tokens]
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Positional Encoding
    position_ids = np.arange(len(tokens))
    positional_embeddings = pe[position_ids]
    combined_embeddings = word_embeddings + positional_embeddings
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ batch
    batch_input = combined_embeddings[np.newaxis, :, :]
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Transformer Block
    output, attention_weights = transformer_block.call(batch_input)
    
    return {
        'tokens': tokens,
        'word_embeddings': word_embeddings,
        'output': output[0],  # Ø­Ø°Ù Ø¨Ø¹Ø¯ batch
        'attention_weights': attention_weights[0]  # Ø­Ø°Ù Ø¨Ø¹Ø¯ batch
    }

# =============================================================================
# ØªØ¬Ø³Ù… Attention Weights
# =============================================================================
def visualize_attention(attention_weights, words, head_idx=0):
    """ØªØ¬Ø³Ù… ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ attention Ø¨Ø±Ø§ÛŒ ÛŒÚ© head"""
    if len(attention_weights) <= head_idx:
        print(fa(f"Head {head_idx} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯"))
        return
        
    plt.figure(figsize=(10, 8))
    plt.imshow(attention_weights[head_idx], cmap='viridis', aspect='auto')
    plt.colorbar()
    plt.title(f'Attention Weights - Head {head_idx+1}')
    plt.xlabel('Key Position')
    plt.ylabel('Query Position')
    plt.xticks(range(len(words)), words, rotation=45)
    plt.yticks(range(len(words)), words)
    plt.tight_layout()
    plt.show()

# =============================================================================
# ØªØ­Ù„ÛŒÙ„ ØªØºÛŒÛŒØ±Ø§Øª Ù¾Ø³ Ø§Ø² Transformer Block
# =============================================================================
def analyze_transformation(original, transformed, words):
    """ØªØ­Ù„ÛŒÙ„ ØªØºÛŒÛŒØ±Ø§Øª Ù¾Ø³ Ø§Ø² Transformer Block"""
    print(fa("ğŸ“Š ØªØ­Ù„ÛŒÙ„ ØªØºÛŒÛŒØ±Ø§Øª:"))
    print("=" * 50)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª
    changes = np.abs(transformed - original)
    
    print(fa(f"Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØºÛŒÛŒØ±Ø§Øª: {np.mean(changes):.6f}"))
    print(fa(f"Ù…Ø§Ú©Ø²ÛŒÙ…Ù… ØªØºÛŒÛŒØ±Ø§Øª: {np.max(changes):.6f}"))
    print(fa(f"Ù…ÛŒÙ†ÛŒÙ…Ù… ØªØºÛŒÛŒØ±Ø§Øª: {np.min(changes):.6f}"))
    
    # ØªØºÛŒÛŒØ±Ø§Øª per token
    print(fa("\nØªØºÛŒÛŒØ±Ø§Øª per token:"))
    for i, word in enumerate(words):
        token_change = np.mean(changes[i])
        print(f"  {word}: {token_change:.6f}")

# =============================================================================
# Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¨Ø§Ù‡Øªâ€ŒÙ‡Ø§ÛŒè¯­ä¹‰
# =============================================================================
def analyze_semantic_similarity(embeddings, words):
    """Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¨Ø§Ù‡Øªâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª"""
    print(fa("\nğŸ” ØªØ­Ù„ÛŒÙ„ Ø´Ø¨Ø§Ù‡Øªâ€ŒÙ‡Ø§ÛŒ:"))
    print("=" * 50)
    
    for i, word1 in enumerate(words):
        for j, word2 in enumerate(words[i+1:], i+1):
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ cosine similarity
            vec1 = embeddings[i]
            vec2 = embeddings[j]
            similarity = np.dot(vec1, vec2) / (
                np.linalg.norm(vec1) * np.linalg.norm(vec2)
            )
            print(f"  {word1} - {word2}: {similarity:.3f}")

# =============================================================================
# Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§ØµÙ„ÛŒ
# =============================================================================
print("\n" + "="*60)
print(fa("Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§ØµÙ„ÛŒ"))
print("="*60)

sentence = "The cat chased the mouse"
results = process_sentence_with_transformer(
    sentence, vocab, embedding_layer, pe, transformer_block
)

if results:
    print(fa("Ø¬Ù…Ù„Ù‡:"), sentence)
    print("Tokens:", results['tokens'])
    words = [reverse_vocab.get(t, 'UNK') for t in results['tokens']]
    print("Words:", words)
    print("Output shape:", results['output'].shape)
    
    # ØªØ¬Ø³Ù… Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ headÙ‡Ø§
    for i in range(min(num_heads, len(results['attention_weights']))):
        visualize_attention(results['attention_weights'], words, i)
    
    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
    original_embeddings = results['word_embeddings'] + pe[:len(results['tokens'])]
    transformed_embeddings = results['output']
    
    analyze_transformation(original_embeddings, transformed_embeddings, words)
    
    print(fa("Ù‚Ø¨Ù„ Ø§Ø² Transformer Block:"))
    analyze_semantic_similarity(original_embeddings, words)
    
    print(fa("\nØ¨Ø¹Ø¯ Ø§Ø² Transformer Block:"))
    analyze_semantic_similarity(transformed_embeddings, words)
else:
    print(fa("Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ù‡"))

# =============================================================================
# ØªØ³Øª Ø¨Ø§ Ø¬Ù…Ù„Ø§Øª Ù…Ø®ØªÙ„Ù
# =============================================================================
print("\n" + "="*60)
print(fa("ØªØ³Øª Ø¨Ø§ Ø¬Ù…Ù„Ø§Øª Ù…Ø®ØªÙ„Ù"))
print("="*60)

test_sentences = [
    "The cat sleeps",
    "The dog runs", 
    "The mouse eats cheese",
    "A bird sings beautifully"
]

for test_sentence in test_sentences:
    print(f"\n{'='*60}")
    print(fa(f"Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ù‡: '{test_sentence}'"))
    print(f"{'='*60}")
    
    try:
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ù‡
        results = process_sentence_with_transformer(
            test_sentence, vocab, embedding_layer, pe, transformer_block
        )
        
        if results:
            words = [reverse_vocab.get(t, 'UNK') for t in results['tokens']]
            print(fa(f"Ú©Ù„Ù…Ø§Øª: {words}"))
            print(fa(f"Ø®Ø±ÙˆØ¬ÛŒ shape: {results['output'].shape}"))
            
            # Ø¨Ø±Ø±Ø³ÛŒ ØªØºÛŒÛŒØ±Ø§Øª
            original = results['word_embeddings'] + pe[:len(results['tokens'])]
            transformed = results['output']
            avg_change = np.mean(np.abs(transformed - original))
            print(fa(f"Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØºÛŒÛŒØ±Ø§Øª: {avg_change:.6f}"))
        else:
            print(fa("Ø¬Ù…Ù„Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø´Ø¯"))
            
    except Exception as e:
        print(f"Ø®Ø·Ø§: {e}")

print(fa("\nğŸ‰ ØªÙ…Ø§Ù… ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!"))