#                                                                             Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§
# install: pip install --upgrade arabic-reshaper
import arabic_reshaper
#-------------------------------------------------------
# install: pip install python-bidi
from bidi.algorithm import get_display
#-------------------------------------------------------
import torch  #  Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§ØµÙ„ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚
import torch.nn as nn
import numpy as np # Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¹Ø¯Ø¯ÛŒ
import matplotlib.pyplot as plt 
import math

# --------------------------------------------------------------------------------------------------------
# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø± Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ù¾Ø§ÛŒØªÙˆÙ† Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
# Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆÙ‚Ø¹ÛŒØª Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
# ØªØ±ØªÛŒØ¨ Ú©Ù„Ù…Ø§Øª: Ø¨Ø¯ÙˆÙ† positional encodingØŒ Ù…Ø¯Ù„ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ØªÙØ§ÙˆØª Ø¨ÛŒÙ† "Ú¯Ø±Ø¨Ù‡ Ø³Ú¯ Ø±Ø§ Ù…ÛŒâ€ŒØ²Ù†Ø¯" Ùˆ "Ø³Ú¯ Ú¯Ø±Ø¨Ù‡ Ø±Ø§ Ù…ÛŒâ€ŒØ²Ù†Ø¯" Ø±Ø§ Ø¯Ø±Ú© Ú©Ù†Ø¯
# --------------------------------------------------------------------------------------------------------

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´ ÙØ§Ø±Ø³ÛŒ
def fa(text):
    return get_display(arabic_reshaper.reshape(text))

# 1. Ú©Ù„Ø§Ø³ Positional Encoding
# Ø§ÛŒÙ† Ø§Ù„Ú¯Ùˆ Ø¨Ù‡ Ù…Ø¯Ù„ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ù‡Ù… Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ø·Ù„Ù‚ Ùˆ Ù‡Ù… Ù…ÙˆÙ‚Ø¹ÛŒØª Ù†Ø³Ø¨ÛŒ Ú©Ù„Ù…Ø§Øª Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§ØªØ±ÛŒØ³ positional encoding
        pe = torch.zeros(max_len, d_model)
        
        #  # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ (0 ØªØ§ max_len-1)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # Ù…Ø­Ø§Ø³Ø¯Ù‡ divisor term
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        # Ø§Ø¹Ù…Ø§Ù„ Ø³ÛŒÙ†ÙˆØ³ Ùˆ Ú©Ø³ÛŒÙ†ÙˆØ³
          # Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø§Ø² ÙØ±Ù…ÙˆÙ„ Ø³ÛŒÙ†ÙˆØ³ Ùˆ Ú©Ø³ÛŒÙ†ÙˆØ³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¨Ù‡ Ù‡Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¯Ø± Ø¬Ù…Ù„Ù‡ ÛŒÚ© Ø§Ù…Ø¶Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨Ø¯Ù‡Ø¯.
        pe[:, 0::2] = torch.sin(position * div_term)  # Ø¨Ø±Ø§ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÙˆØ¬ Ø§Ø² Ø³ÛŒÙ†ÙˆØ³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        pe[:, 1::2] = torch.cos(position * div_term)  # Ø¨Ø±Ø§ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ±Ø¯ Ø§Ø² Ú©Ø³ÛŒÙ†ÙˆØ³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ø¹Ø¯ batch
        pe = pe.unsqueeze(0)
        
        # Ø«Ø¨Øª Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† buffer (Ù†Ù‡ parameter)
        self.register_buffer('pe', pe)
    
    # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆÙ‚Ø¹ÛŒØª Ø±Ø§ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    def forward(self, x):
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† positional encoding Ø¨Ù‡ embedding
        return x + self.pe[:, :x.size(1)]

# 2. ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª
def generate_sample_data(seq_length=50, d_model=64, batch_size=16):
    # Ø§ÛŒØ¬Ø§Ø¯ embedding Ù‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ
    embeddings = torch.randn(batch_size, seq_length, d_model)
    return embeddings

# 3. Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ùˆ ØªØ³Øª
d_model = 64      # ØªØ¹Ø¯Ø§Ø¯ Ø§Ø¨Ø¹Ø§Ø¯ Ù‡Ø± Ø¨Ø±Ø¯Ø§Ø±
seq_length = 20   # Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡
batch_size = 8    # ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù…Ù„Ø§Øª Ø¯Ø± Ù‡Ø± Ø¯Ø³ØªÙ‡

# Ø§ÛŒØ¬Ø§Ø¯ positional encoding
pos_encoder = PositionalEncoding(d_model)

# ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
sample_embeddings = generate_sample_data(seq_length, d_model, batch_size)

# Ø§Ø¹Ù…Ø§Ù„ positional encoding
encoded_embeddings = pos_encoder(sample_embeddings)

print(fa("Ø´Ú©Ù„ embeddings ÙˆØ±ÙˆØ¯ÛŒ:"), sample_embeddings.shape)
print(fa("Ø´Ú©Ù„ embeddings Ø®Ø±ÙˆØ¬ÛŒ:"), encoded_embeddings.shape)

# 4. ØªØ¬Ø³Ù… positional encoding
# Ù†Ù…Ø§ÛŒØ´ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ú†Ú¯ÙˆÙ†Ù‡ Ù‡Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª Ø§Ù„Ú¯ÙˆÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ÛŒ Ø¯Ø§Ø±Ø¯
def visualize_positional_encoding(d_model=64, max_len=100):
    pe = torch.zeros(max_len, d_model)
    
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                       (-math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    plt.figure(figsize=(15, 5))
    
    # Ù†Ù…ÙˆØ¯Ø§Ø± heatmap
    plt.subplot(1, 2, 1)
    plt.imshow(pe.numpy().T, cmap='viridis', aspect='auto')
    plt.colorbar()
    plt.title('Positional Encoding Heatmap')
    plt.xlabel(fa('Ù…ÙˆÙ‚Ø¹ÛŒØª'))
    plt.ylabel(fa('Ø¨Ø¹Ø¯'))
    
    # Ù†Ù…ÙˆØ¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯ Ø¨Ø¹Ø¯ Ø®Ø§Øµ
    plt.subplot(1, 2, 2)
    for i in range(0, 8, 2):
        plt.plot(pe[:, i].numpy(), label=fa(f'Ø¨Ø¹Ø¯ {i}'))
    plt.title(fa('Ù…Ù‚Ø§Ø¯ÛŒØ± Positional Encoding Ø¨Ø±Ø§ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø®ØªÙ„Ù'))
    plt.xlabel(fa('Ù…ÙˆÙ‚Ø¹ÛŒØª'))
    plt.ylabel(fa('Ù…Ù‚Ø¯Ø§Ø±'))
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    return pe


# ØªØ¬Ø³Ù…
pe_matrix = visualize_positional_encoding()

# 5. ØªØ³Øª ØªÙØ§ÙˆØª Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§
def test_position_differences():
    # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ø®ØªÙ„Ù
    pos1, pos2 = 5, 15
    
    plt.figure(figsize=(12, 4))
    
    # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ…
    plt.subplot(1, 2, 1)
    plt.plot(pe_matrix[pos1], 'b-', label=fa(f'Ù…ÙˆÙ‚Ø¹ÛŒØª {pos1}'))
    plt.plot(pe_matrix[pos2], 'r-', label=fa(f'Ù…ÙˆÙ‚Ø¹ÛŒØª {pos2}'))
    plt.title(fa('Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù'))
    plt.xlabel(fa('Ø¨Ø¹Ø¯'))
    plt.ylabel(fa('Ù…Ù‚Ø¯Ø§Ø±'))
    plt.legend()
    
    # ØªÙØ§ÙˆØª Ø¨ÛŒÙ† Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§
    plt.subplot(1, 2, 2)
    difference = pe_matrix[pos1] - pe_matrix[pos2]
    plt.plot(difference, 'g-')
    plt.title(fa('ØªÙØ§ÙˆØª Ø¨ÛŒÙ† Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§'))
    plt.xlabel(fa('Ø¨Ø¹Ø¯'))
    plt.ylabel(fa('ØªÙØ§ÙˆØª'))
    
    plt.tight_layout()
    plt.show()

test_position_differences()

# 6. ØªØ³Øª Ø¯Ø± ÛŒÚ© Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡
# Ù…Ø¯Ù„ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø± Ú©Ø§Ù…Ù„
  # Ø¬Ø±ÛŒØ§Ù† Ø¯Ø§Ø¯Ù‡:
    # Ú©Ù„Ù…Ø§Øª â†’ Ø¨Ø±Ø¯Ø§Ø±
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÙˆÙ‚Ø¹ÛŒØª
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙˆØ³Ø· ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±
    # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
class SimpleTransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        super(SimpleTransformerModel, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        
        # Ù„Ø§ÛŒÙ‡ Transformer
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, nhead),
            num_layers
        )
        
        self.classifier = nn.Linear(d_model, vocab_size)
    
    def forward(self, x):
        # 1. ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±
        x = self.embedding(x)
        
        # 2. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆÙ‚Ø¹ÛŒØª
        x = self.pos_encoder(x)
        
        # 3. Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙˆØ³Ø· ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±
        x = self.transformer(x)
        
        # 4. Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        x = self.classifier(x.mean(dim=1))
        return x

# 7. ØªØ³Øª Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
def test_with_real_data():
    # Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)
    vocab_size = 1000 # Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø§ÛŒØ±Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†
    seq_length = 20   # Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
      # ÛŒÚ©ÛŒ Ø§Ø² Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø± Ø§Ø³Øª.
      # Ù…Ù‚Ø§ÛŒØ³Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§
      #  ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡	|       Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ |	Ù‚Ø¯Ø±Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ |	Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´	| Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²  
      #           ÛŒÚ© |              Ú©Ù… |         Ù…Ø­Ø¯ÙˆØ¯ |         Ø³Ø±ÛŒØ¹ |      Ú©Ù…
      #        Ø¯Ùˆ |           Ù…ØªÙˆØ³Ø· |         Ù…ØªÙˆØ³Ø· |        Ù…ØªÙˆØ³Ø· |      Ù…ØªÙˆØ³Ø·
      #        Ú†Ù‡Ø§Ø± Ø§Ù„ÛŒ Ø´Ø´ |           Ø²ÛŒØ§Ø¯ |            Ù‚ÙˆÛŒ |          Ú©Ù†Ø¯ |       Ø²ÛŒØ§Ø¯
      #     Ø¨ÛŒØ´ØªØ± Ø§Ø² 12 |   Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯	 |     Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ |    Ø¨Ø³ÛŒØ§Ø± Ú©Ù†Ø¯ |   Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯


    model = SimpleTransformerModel(vocab_size, d_model=64, nhead=4, num_layers=2)
    
    # Ø¯Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ÛŒ (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)
    input_data = torch.randint(0, vocab_size, (8, seq_length))  # 8 Ù†Ù…ÙˆÙ†Ù‡ØŒ Ø·ÙˆÙ„ 20
    
    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
    output = model(input_data)
    print(fa("Ø´Ú©Ù„ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„:"), output.shape)
    
    return model, output

model, output = test_with_real_data()

# 8. Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù‡Ù…ÛŒØª Positional Encoding
def demonstrate_importance():
    # Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ø¨Ø§ Ú©Ù„Ù…Ø§Øª ÛŒÚ©Ø³Ø§Ù† Ø§Ù…Ø§ ØªØ±ØªÛŒØ¨ Ù…Ø®ØªÙ„Ù
    sentence1 = torch.tensor([[1, 2, 3, 4]])  # "Ú¯Ø±Ø¨Ù‡ Ø³Ú¯ Ù¾Ø±ÛŒØ¯ Ù…ÛŒÙˆÙ‡"
    sentence2 = torch.tensor([[4, 3, 2, 1]])  # "Ù…ÛŒÙˆÙ‡ Ù¾Ø±ÛŒØ¯ Ø³Ú¯ Ú¯Ø±Ø¨Ù‡"
    
    # Ø¨Ø¯ÙˆÙ† positional encoding
    embedding = nn.Embedding(10, 64)
    emb1 = embedding(sentence1)
    emb2 = embedding(sentence2)
    
    # Ø¨Ø§ positional encoding
    pos_emb1 = pos_encoder(emb1)
    pos_emb2 = pos_encoder(emb2)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
    similarity_no_pos = torch.cosine_similarity(emb1.flatten(), emb2.flatten(), dim=0)
    similarity_with_pos = torch.cosine_similarity(pos_emb1.flatten(), pos_emb2.flatten(), dim=0)
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² f-string Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ù…Ø´Ú©Ù„
    print(fa(f"Ø´Ø¨Ø§Ù‡Øª Ø¨Ø¯ÙˆÙ† Positional Encoding: {similarity_no_pos.item():.4f}"))
    print(fa(f"Ø´Ø¨Ø§Ù‡Øª Ø¨Ø§ Positional Encoding: {similarity_with_pos.item():.4f}"))
    
    # ØªÙØ§ÙˆØª Ø¨Ø§ÛŒØ¯ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ Ø¨Ø§Ø´Ø¯!
    difference = abs(similarity_no_pos - similarity_with_pos)
    print(fa(f"ØªÙØ§ÙˆØª: {difference.item():.4f}"))
    
    if difference > 0.1:
        print(fa("âœ… Positional Encoding Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯!"))
    else:
        print(fa("âš ï¸  Positional Encoding ØªØ§Ø«ÛŒØ± Ú©Ù…ÛŒ Ø¯Ø§Ø±Ø¯"))

demonstrate_importance()

# 9. Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ
print(fa("\nğŸ¯ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Positional Encoding:"))
applications = [
    "Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒ",
    "Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯",
    "Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ",
    "ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†",
    "Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±",
    "Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª"
]

for i, app in enumerate(applications, 1):
    print(f"{i}. {fa(app)}")

# 10. Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
def save_and_load_example():
    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
    torch.save({
        'model_state_dict': model.state_dict(),
        'pos_encoder_state_dict': pos_encoder.state_dict()
    }, 'positional_encoding_model.pth')
    
    print(fa("Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!"))
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
    checkpoint = torch.load('positional_encoding_model.pth')
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ø¬Ø¯ÛŒØ¯
    new_model = SimpleTransformerModel(1000, 64, 4, 2)
    new_model.load_state_dict(checkpoint['model_state_dict'])
    
    new_pos_encoder = PositionalEncoding(64)
    new_pos_encoder.load_state_dict(checkpoint['pos_encoder_state_dict'])
    
    print(fa("Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!"))

save_and_load_example()
